# 08.初识Keras：轻松完成神经网络模型搭建

一路走来我们已经到了第八节课，前面的7节课我们都是在研究关于神经网络的底层基本原理，那么这节课就是我们的分水岭，以后我们就不再进行底层的数学原理分析。对于工程实践而言，目前我们所学的内容已经可以开始了，但是如果你决定踏入机器学习神经网络的大门，并准备衣带渐宽终不悔的深入这个领域，那么希望你自己继续研究其背后的数学原理和该领域目前最新的想法，在此之前我们来聊一聊最后一个稍微麻烦一点的问题。当然或许这并不算是一个问题，而是将一直陪伴我们接下来课程的工具。

## 介绍向量和矩阵

我们在上一节课中说到，随着输入数据的特征越来越多，如果一个个的去编写函数表达式未免有点麻烦和拖沓，所以我们需要一个数学工具让，这件事情变得简单，矩阵和向量，这是一个向量，这是一个矩阵，这么大一坨看着好吓人，呐但有些事情就是这样，往往越看着麻烦和吓人的东西呢越简单，而而看着简洁的东西却蕴含着复杂向量和矩阵到底是什么？很多人尝试从不同的角度去解释这个问题。​

物理学家会说是空间变换，数据工程师说是数据特征，程序员们会说是数组，办公人士看见了多半会说唉这不就是Excel表格的行和列，嘛不同领域的人对他的理解各不相同，一旦我们偏向于某个角色去认识他，就会因为其他角色的声音让我们困惑，我们更需要用数学家的眼光来看待他。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815162549-dwrt6mn.png)​

矩阵和向量它就是一个工具，正如数学是服务其他科学的工具一样，它是数学的工具，我们最开始学数学的时候接触的是最简单的一元一次函数，后来我们又接触到二元一次函数，如果你对初中数学还有印象，那么鸡兔同笼的问题就在这里。

后来我们又接触到3元、4元、5元等等，我们发现原来这个原本可以无限地拖下去，这时候向来有洁癖的数学家们就跳出来了，说到这不够优雅也不方便，不然我们发明一个工具，于是向量出现了。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815162602-98kntsv.png)​

我们用三元一次函数举例，用一个向量把这个函数的自变量放入其中，再把权重系数放入到另一个向量中，把偏置、项系数b也放入一个向量中，基本的表示方法有了，那么就在规定一些运算的规则，我们给向量定一个转制的规则，在向量上写个t表示对它进行转制，转制后向向量也就从一行竖成了一列，转制的操作可以简单的理解为把躺着的一行支楞起来变成一列，那再规定一下向量之间的乘法。

向量x点成w的转置，咱们这样算横的和竖的对应的元素相乘再加起来。当然这是向量和向量的运算，所以这个运算的结果也应该是个向量，这样才统一，虽然这个向量只有一个元素，但如果我们用万物皆向量的世界观去看待问题，把单个的数字看做一个向量也就不奇怪了，最后我们让计算的结果再加上偏执向量b根据向量的加法，最后得到的结果是这样的，这个计算的结果恰好就是我们函数的运算结果，你看这样就可以使用向量的运算去表示线性函数了，x是输入向量，w是权重系数向量b b是偏执项向量，z是结果向量，这难道不比我们写z等于w1×x1+w2×x2加上w3×x3+b要简洁许多吗？

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815162643-fawyk1c.png)​

当然三个输入或许你还感受不到这种简洁到底有多棒，那如果是一个1000元的一次函数，如果不用向量的手段那可能需要这么写，z等于w1乘以x加上ywo以x2加w3×x加上w4×x加上等等，一直加到w1000×x1000家加b我们只能用省略号，否则一页纸估计是写不完的，但如果我们使用向量运算，那么这个1000元一次线性函数仍就是z等于x点成w的转置加上b只不过这时候我们需要清楚s和w的元素数量都是1000，有时候向量的元素的数量也叫维度。​

这一点大家在中学时代学向量的时候应该很清楚，我们用一个二维向量表示二维平面的一个矢量，用三维向量表示三维空间中的一个矢量。当然我们也可以用一个四维向量表示四维空间的矢量，只不过作为三维生物，我们已经无法直观的去感受它的样子了，所以不再具备物理意义而成为了纯粹的数学手段。但不管这个线性函数是多少元，都可以采用这种向量运算的方式表示这个函数。输入是向量权重参数和偏置，参数也是向量计算，结果输出也是向量。

那从这种角度去考虑，我们最开始的单层进元单输入的模型是这样的，x是一个一维向量 w是一个一维向量b也是一个一维向量结，我z也是一个一味香料，而对于我们上节课中提到的两个维度的输入数据模型，我们自然也可以用向量运算来表示，这时候x和w呢都是一个二维向量，我们不需要再写z等于w1×x一加上 w二请si加上b这种繁琐的函数设置了，同样如果我们把豆豆的外壳硬度作为第三个输入的特征维度，那我们还是可以使用向量运算建立模型。

这时候x和w都是一个三维向量，我们不需要再写z等于w1×x1+w2×x2+w3×x3+b这种繁琐的函数式子了，这让事情变得很简洁很统一很nice。但追求极致的数学家们又发现，如果输入的数据送入的是一组函数进行运算，比如我们加入两个隐藏层神经元的神经网络，那输入实际上就是分别送入到两个线性函数进行计算，这时候单靠向量似乎也不是很酸爽，每一个函数都有一个权重系数向量和偏执系数向量，那为什么不把它们放在一起呢与？

于是矩阵出现了。同样这个矩阵也有转置操作，其实你大可以把矩阵看作是由多个向量并在一起形成的特殊向量，所以矩阵的转置就可以简单的看作是对其中每一个向量进行转置操作，第一行竖起来变成第一列，第二行竖起来变成第二列转制以后，我们再让输入向量x点成这个系数矩阵，而输入向量x和这个权重系数矩阵的点乘运算，也可以看作是分别让输入向量x点成第一列，计算出第一个计算结果，再让输入向量x点成第二列，计算出第二个结果，这两个计算的结果组成一个结果向量，再让这个结果向量加上偏执向量b你看最终的结果向量中的两个元素分别就是这两个函数的运算结果，所以你会发现此时对于多个函数，我们仍然可以使用z等于x乘以w的转置加b来表示，只不过我们现在要明确w是一个2×3的矩阵，没错，矩阵和向量就是能够把事情做得如此的简洁和统一，不论输入的数据是多少维度，也不管输入的数据送了多少个神经元做线性函数运算，数学形式都被统一称为一模一样，如此大势成瘾，数学家爽了，那程序员能不能也爽？呢当然可以。像Python中的numpy这种数学库在做矩阵和向量计算的时候，就和这里的数学符号一样非常的简洁和方便，那我们就在变成实验中一探究竟。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815162829-knxl7rv.png)​

## 利用向量和矩阵进行编程

好的，同学们，我们来做本节课的第一个编程实验，那看看我们如何用向量和矩阵运算的方式呢去构建我们的神经元。

两个输入一个神经元

```python
import numpy as np
import utils.dataset as dataset
import utils.plot_utils as plot_utils
# %matplotlib
%matplotlib ipympl

## Create a dataset
n = 100
X, Y = dataset.get_beans5(n)

# 输入xs是二维向量
# print(X) 
# print(Y)
plot_utils.show_scatter(X,Y)


W = np.array([0.1,0.1])
B = np.array([0.1])


def forward_propgation(X):
    Z= X.dot(W.T)+B
    A = 1/(1+np.exp(-Z))
    return A


plot_utils.show_scatter_surface(X,Y, forward_propgation)

for _ in range(500):
    for i in range(n):
        Xi = X[i]
        Yi = Y[i]

        # 前向传播
        A = forward_propgation(Xi)
        E = (Yi-A)**2
        # 反向传播
        dEdA = -2*(Yi-A)
        dAdZ = A*(1-A)
        dZdW = Xi
        dZdB = 1

        dEdW = dEdA*dAdZ*dZdW
        dEdB = dEdA*dAdZ*dZdB

        # 梯度下降
        alpha = 0.01
        W = W - alpha*dEdW
        B = B - alpha*dEdB
plot_utils.show_scatter_surface(X, Y, forward_propgation)

```

两个输入一层隐藏层两个神经元

```python
import dataset
import plot_utils

import numpy as np
#从数据中获取随机豆豆
m=100
X,Y = dataset.get_beans4(m)
print(X)
print(Y)

plot_utils.show_scatter(X, Y)

W1 = np.random.rand(2,2)
B1 = np.random.rand(1,2)
W2 = np.random.rand(1,2)
B2 = np.random.rand(1,1)


def forward_propgation(X):

	#Z1:(m,2)
	Z1 = X.dot(W1.T) + B1

	#A1:(m,2)
	A1 = 1/(1+np.exp(-Z1))
	#Z2:(m,1)
	Z2 = A1.dot(W2.T) + B2
	#A2:(m,1)
	A2 = 1/(1+np.exp(-Z2))
	return A2,Z2,A1,Z1

plot_utils.show_scatter_surface(X, Y,forward_propgation)


for _ in range(5000):
	for i in range(m):
		Xi = X[i]

		Yi = Y[i]
		A2,Z2,A1,Z1 = forward_propgation(Xi)


		E = (Yi - A2)**2


		#(1,1)
		dEdA2 = -2*(Yi-A2)
		#(1,1)
		dEdZ2 = dEdA2*A2*(1-A2)

	

		#(1,2)
		dEdW2 = dEdZ2*A1
		#(1,1)
		dEdB2 = dEdZ2*1

		#(1,2)
		dEdA1 = dEdZ2*W2

		#(1,2)
		dEdZ1 = dEdA1*A1*(1-A1)


		dEdW1 = (dEdZ1.T).dot(np.array([Xi]))




		dEdB1 = dEdZ1*1

	

	
		alpha = 0.05
		W2 = W2 - alpha*dEdW2
		B2 = B2 - alpha*dEdB2

		W1 = W1 - alpha*dEdW1

		B1 = B1 - alpha*dEdB1
	#计算准确率

	A2,Z2,A1,Z1 = forward_propgation(X)
	A2 = np.around(A2)#四舍五入取出0.5分割线左右的分类结果
	A2 = A2.reshape(1,m)[0]
	accuracy = np.mean(np.equal(A2,Y))
	print("准确率："+str(accuracy))

plot_utils.show_scatter_surface(X, Y,forward_propgation)














```

## keras框架介绍

欢迎回来，从本节课开始，为了能够快速的上手实践，我们就不再手写神经网络的实现，而使用机器学习框架keras让他帮我们彻底摆脱各种底层麻烦的事情，所以keras框架到底是什么？对于有计算机基础的同学来说，我们可以做一个类比进行理解，我们用汇编语言写程序也是可以的，可是那太麻烦了，你必须完全了解你所面对的机器有多少的寄存器，run有多少，怎么分布的，如何响应中断等等。所以人们发明的c语言一下子就让我们很大程度上摆脱了计算机底层硬件的琐碎问题，而能专注于编写我们想要的功能。

当然在使用c语言的时候，我们必要时还需要考虑一些d层的琐碎问题，人们希望编程更简单，更注重业务，而不是机器，所以出现了Java、c++等等这些更加上层的语言。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815163419-pld9zat.png)​

直到Python、JS这种脚本语言的出现，我们几乎不太需要关心底层到底发生了什么，keras框架就向机器学习里的高级语言实现了对机器学习神经网络底层复杂的数据运算的封装，我们可以轻松的通过它提供了各种上层接口搭建模型，当然除了keras以外，还有很多其他的框架，比如最耳熟能详的tensorflow，还是用编程语言来举例子，tensorflow更像是c语言对底层的封装并不是那么的完全，但是更加的强大和灵活，而keras更像是python，就两个字简单。

我们可以类比一下，一个同样的神经元模型，用keras和tensorflow架实现的代码量，你会很直观的看见它们的区别，但不论keras如何简单，我们也不能在完全不懂底层原理的情况下去使用它，就像不论你使用c语言还是python语言编程，都断然不可完全不懂计算机基本原理一样，或许你能做出一点东西，但最终会很虚无缥缈，也很难向更高的地方前进。​

既然keras框架如此神奇，那我们就来看看如果用他来搭建小蓝的大脑和之前相比是怎样的一个过程，之前即使为了实现一个简单的神经元，也需要编写各种繁琐的代码，首先是前向传播，然后是计算代价函数，再然后利用反向传播计算误差，在每个神经元上的权重和偏置参数的导数进行梯度下降，这些过程想想就很繁琐。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815163543-p710wpc.png)​

而利用keras框架事情就变成了这个样子。首先导入keras，然后创建一个模型，再然后创建一个神经元使用sigmoid激活函数，最后告诉keras使用均方误差代价函数和随机梯度下降算法，如此就可以开始训练了，就这么简单现在如果我们想要把神经元变成两个，你只需要把units等于1改成units等于2就可以了。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815163548-ta9pxmf.png)​

当然我们的豆豆分类作为一个分类问题，只有一个0或1的单输出，两个神经元有两个输出，所以我们需要在后面再加一个神经元，把这两个神经元会合一下。如果你想让这个模型更加复杂也更强大一点，比如把第一层的神经元数量改成4，同样也只需要简单的把units设置为4即可，这和我们手撸模型相比简直太酸爽了。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815163617-jub9h2b.png)​

这就是我们的keras框架，作为机器学习神经网络的应用入门，它的极致简单，简直就是福音。我们打开keras的文档，正如他的第一段话说的那样，你恰好发现了keras，那说完keras的酸爽，我们也简单的介绍一下他存在的一些问题。

首先正如文档里说的那样，keras并不是一个独立的框架，而是通过调用诸如tensorflow、CNTK或Theano等独立的框架实现的，你可以简单的类比变成Python语言的运行环境是用更底层一点的c或c++写出来的。

第二就因为它太简单了，封装的太好了，所以有时候并没有像更加底层的tensorflow那样灵活，虽然他说高度模块化可扩展性，但众所周知，往往高度封装带来的简单会造成对具体细节控制的流失。当然经过这么多年的发展，keras在灵活性上也在不断的增强，而作为同出谷歌之手的两大框架，目前也已经把keras并入到了泰森弗洛之中，作为tensorflow的高级API，但问题是如果我使用keras去做这么细致的事情，那为什么不使用tensorflow呢？正如我们在面对谁是最好的编程语言，这个问题一样，答案是一致的，没有最好，只有最合适。

## 使用keras框架进行编程​

好的，同学们，我们来做本节课的第二个编程实验，那使用keras呢搭建我们的神经网络模型，在这个部分的编码中，我们把事情搞大一点，使用keras呢快速的实现目前为止我们做过的所有模型，**从第五节课用单神经元预测输入数据特征唯一的简单分类问题，再到第六节课加入隐藏层的神经网络预测输入数据特征唯一的复杂分类问题，再到我们上节课用单神经元预测数据特征为二的简单分类问题，当然还有上节课就说过了，但是到目前为止我们还没有实现的加入隐藏层的神经网络去预测数据特征为二的复杂问题**，那其实呢也不算多大的事情，因为就像我们说的keras的设计呢非常的简洁，其实呢也就稍微改变某些参数就可以了。

* **准备上手keras框架**

  * **首先安装keras和tensorflow**：keras依赖于tensorflow，只需要安装tensorflow，就会自动安装keras。

    ```bash
    pip install -U tensorflow
    ```
  * **查看版本**：tensorflow 2.9.1 | keras 2.9.0

    ```shell
    $ pip show tensorflow
    Name: tensorflow
    Version: 2.9.1
    Summary: TensorFlow is an open source machine learning framework for everyone.
    Home-page: https://www.tensorflow.org/
    Author: Google Inc.
    Author-email: packages@tensorflow.org
    License: Apache 2.0
    Location: d:\environment\miniconda\envs\ai\lib\site-packages
    Requires: flatbuffers, h5py, libclang, keras-preprocessing, gast, grpcio, protobuf, absl-py, tensorflow-estimator, google-pasta, packaging, tensorboard, tensorflow-io-gcs-filesystem, wrapt, numpy, astunparse, opt-einsum, termcolor, typing-extensions, keras, setuptools, six
    Required-by:
    $ pip show keras
    Name: keras
    Version: 2.9.0
    Summary: Deep learning for humans.
    Home-page: https://keras.io/
    Author: Keras team
    Author-email: keras-users@googlegroups.com
    License: Apache 2.0
    Location: d:\environment\miniconda\envs\ai\lib\site-packages
    Requires:
    Required-by: tensorflow
    ```
  * **keras与tensorflow**：keras=python，tensorflow=x

    * **keras框架与tensorflow**：keras框架就向机器学习里的高级语言实现了对机器学习神经网络底层复杂的数据运算的封装，我们可以轻松的通过它提供了各种上层接口搭建模型。而tensorflow更像是c语言对底层的封装并不是那么的完全，但是更加的强大和灵活，而keras更像是python，就两个字简单。
    * 一个同样的神经元模型，用keras和tensorflow架实现的代码量，你会很直观的看见它们的区别

      ![image](assets/image-20220815163419-pld9zat.png)​
* **keras实现含有一层隐藏层的神经网络代码**

  * **代码**

    ```python
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.optimizers import SGD

    # 创建模型
    model = Sequential() 
    # 创建一层网络，即隐藏层，含有两个神经元，激活函数使用sigmoid函数，输入特征为2维
    model.add(Dense(units=2, activation='sigmoid',input_dim=2 ))
    # 再创建一层网络，即输出层，含有一个神经元，激活函数使用sigmoid函数
    model.add(Dense(units=1, activation='sigmoid' ))
    # 告诉keras使用均方误差代价函数和随机梯度下降算法SGD，学习率为0.05，评估指标选择accuracy
    model.compile(loss='mse', optimizer=SGD(learning_rate=0.05),metrics=['accuracy'])
    # 开始训练
    model.fit(X, Y, epochs=5000, batch_size=10,verbose=0)
    # 训练完毕，进行预测
    pres= model.predict(X)
    ```
  * 概念解释：Sequential、Dense、activation、optimizer

    * Sequential：简单来说就是一个用来堆叠神经网络的序列，深度神经网络其实就是一层一层堆叠出来的，所以你可以认为这个Sequential呢就是用来堆叠神经网络的，那这些神经元堆叠在一起形成一个神经网络预测模型。
    * Dence：一个全连接层，简单来说就是一层的神经网络。全连接这三个字的意思呢，就是字面的意思，这一层的每个神经元的输入和输出和上一层和下一层的每一个都全连接着，一个都不少，那我们目前为止搭建的都是这种全连接神经网络。
    * optimizer表示优化器，也就是说用来优化或者说调整参数的算法，目前我们就指定使用随机梯度下降算法
* **用keras框架快速实现课程前面的几个神经网络**​

  * 用单神经元预测输入数据特征为1的简单分类问题  
    ​![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220811205119-oz4whbg.png)​

    ```python
    model = Sequential()
    model.add(Dense(units=1, activation='sigmoid',input_dim=1 ))
    model.compile(loss='mse', optimizer='sgd',metrics=['accuracy'])
    model.fit(X, Y, epochs=5000, batch_size=10,verbose=0)
    ```
  * 加入隐藏层的神经网络预测输入数据特征为1的复杂分类问题![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220811205130-9xpcy81.png)​

    ```python
    model = Sequential() # 创建模型
    model.add(Dense(units=2, activation='sigmoid',input_dim=1 ))
    model.add(Dense(units=1, activation='sigmoid' ))
    model.compile(loss='mse', optimizer=SGD(learning_rate=0.05),metrics=['accuracy'])
    model.fit(X, Y, epochs=5000, batch_size=10,verbose=0)
    ```
  * 用单神经元预测数据特征为2的简单分类问题

    ![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220811210653-7uoa4bz.png)​

    ```python
    model = Sequential()
    model.add(Dense(units=1, activation='sigmoid',input_dim=2))
    model.compile(loss='mse', optimizer=SGD(learning_rate=0.05),metrics=['accuracy'])
    model.fit(X, Y, epochs=5000, batch_size=10,verbose=0)
    ```
  * 加入隐藏层的神经网络去预测数据特征为2的复杂问题

    ![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220811210536-hjtkgxy.png)​

    ```python
    model = Sequential()
    model.add(Dense(units=2, activation='sigmoid',input_dim=2)) # 隐藏层
    model.add(Dense(units=1, activation='sigmoid')) # 输出层
    model.compile(loss='mse', optimizer=SGD(learning_rate=0.05),metrics=['accuracy'])
    model.fit(X, Y, epochs=5000, batch_size=10,verbose=0) # 训练
    ```
