# 06.隐藏层：神经网络为什么working

大自然往往是变幻莫测，喜怒无常。在一次地球环境巨变之后，小蓝所在的海底生物们也经历了巨大的进化，豆豆变得不再是简单的越大或者越小越可能有毒，而是在某个大小范围内有毒，而某些范围内无毒。

比如这样，此时不论是不加激活函数的预测模型，还是加了激活函数的预测模型，似乎都开始变得无能为力了。因为从数学上来讲，这些函数在任意范围内的单调性是一致的，要么单调增，要么处单调减，而新的豆豆的毒性却变得忽大忽小，想要预测它们只能靠一些不那么单调的函数，比如这样，**那么如何让预测模型能够产生这种山丘一样的曲线呢？**
![20220815161304](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/06.隐藏层：神经网络为什么working/20220815161304.png) 

**是时候让神经元形成一个网络了**。我们唯一做的事情就是多添加了两个神经元，并把输入分别送入到这两个神经元进行计算，再把计算的结果送入到第三个神经元计算，最后输出。我们用可视化工具调节一下这三个神经元的参数，最后得到这样的一个效果分类成功，所以为什么会这样？

![20220815161312](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/06.隐藏层：神经网络为什么working/20220815161312.png)​

其实很简单，对于第一个神经元先通过线性函数计算，再通过激活函数得到最终的输出，而利用梯度下降算法，最终的输出一定可以被调节成为现在这个样子。第二个神经元也同样最终被调节成为这个样子，把这两个神经元的最终输出作为第三个神经元的输入，先通过第三个神经元的线性函数计算乘以权重之后变成这个样子，求和之后是这样的。这时候我们发现这个神经元的线性运算的结果已然是一个有起伏的不再单调的曲线了，再通过激活函数并通过梯度下降算法，把最终的输出调节成为这这个样子，这就是我们想要的结果。也就是说把输入分为两个部分，然后分别对这两个部分进行调节，然后再送入最后一个神经元让整体的神经网络形成一个单调性不唯一的更多变的函数，从而具备了解决更加复杂问题的能力。

![20220815161325](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/06.隐藏层：神经网络为什么working/20220815161325.png)​

当然这里由于每个神经元都有一个偏执向b也就是线性函数的截距，一般认为这是大家默认的共识，所以为了在画网络结构图的时候不那么的拖沓，大家一般都会直接省略这些b让我们的网络结构图更加的精简。

这就是数学上的解释很无趣，对吧？

当然这里还有一个解释，虽然严谨性并不是那么好，但似乎更加有序，我们添加一个神经元后，相当于增加了一个抽象的维度，把输入放进这些不同的维度中，每个维度通过不断的调整权重并进行激活，从而产生对输入的不同理解。最后再把这些抽象维度中的输出进行合并，降为得到输出。这个输入数据由于在多个抽象维度中被产生了不同的解读，从而让输出得到了更多的可能。

同样当环境中的豆豆的毒性发生更多可能的时候，我们同样可以采用类似的方法，通过增加对输入的更多抽象纬度产生更多的解读，而实现更加复杂的分的效果，而中间这些新添加的神经元节点也称之为**隐藏层**。可以看出来**正是隐藏层的存在才让神经网络能够在复杂的情况下仍旧working**，显而易见的是**隐藏层的神经元数量越多，就可以产生越复杂的组合，解决越复杂的问题，当然计算量也随之越来越大**，我们已经横向的在神经网络上增加了神经员，形成了一层隐藏层，而在之后的课程中，我们会纵向的不断添加神经元，产生更深的隐藏层输入，通过这些隐藏层被一层又一层的不断抽象和理解，提取出微妙的特征，从而让神经网络变得更加强大和智能。

![20220815161338](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/06.隐藏层：神经网络为什么working/20220815161338.png)
而这些隐藏层也就是神经网络为什么working的本质，当我们建立一个复杂程度恰当的神经网络，经过充分的训练之后，网络中的各个神经元的参数被调节成为不同的值，这些功能单一的神经元连接组合出来的整体就可以近似出一个相当复杂的函数，可能是这样这样或者是这样，最终变成什么样，则是根据我们采集的训练数据决定，如果训练数据只有这样的两个，那么最终的预测模型可能是这样，如果是这样的数据集最后可能会是这样

如果是更多的这样的数据集则结果可能会是这样
![20220815161350](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/06.隐藏层：神经网络为什么working/20220815161350.png)

而我们采集的训练数据越充足，那么最后训练得到的模型也就能够越好的去预测新的问题，因为越充足的训练数据就在越大程度上蕴藏了问题的规律和特征，新的问题数据也就越难以逃脱这些规律的约束。

所以我们总是说机器学习神经网络的根基是海量的，一个训练之后拟合适当的模型，进而在遇到新的问题数据的时候，也能大概率产生正确的预测。我们把这个现象称之为**模型的泛化**，模型的泛化能力，也就是神经网络追求的核心问题。​

我们经常听到深度学习，其中深度二字其实并没有什么特别的奥义，**只是指一个神经网络中纵向的隐藏层比较多**，换句话说很深，我们一般把隐藏层超过三层的网络也就称之为深度神经网络，这也是深度学习中广受诟病的地方，隐藏层的神经元在理解什么提取什么都太过微妙，虽然我们对大致的结果有所把握，但却很难用精确的数学去进行描述，我们能做的也只有设计一个网络送入数据，然后充分的训练，如果得到的预测效果好，我们就会说它起作用，如果不好，那也只能说搞错了，调校参数在再来一遍。

![20220815161406](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/06.隐藏层：神经网络为什么working/20220815161406.png)
所以很多人戏称深度学习是炼丹，确实道士把原材料放入八卦炉开火炼丹，最后得到的仙丹可能让人长生不老，也可能让人一命呜呼。炼丹过程中八卦炉里发生的微妙事情，道士也是不得而知的，虽然是他设计了炼丹的一切。
![20220815161416](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/06.隐藏层：神经网络为什么working/20220815161416.png)

## 编程实验

好的，同学们，我们开始做本节课的编程实验，那提前说一下这节课的编码呢可能会有一点繁琐，但并不是说多么难，大家要分清楚繁琐和难是两回事儿，不过不必慌张，我们沉下心来慢慢的梳理。​​

![20220815161429](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/06.隐藏层：神经网络为什么working/20220815161429.png)​

初始化数据集

```python
## Create a dataset
n = 100
xs, ys = dataset.get_beans4(n)
```

初始化参数，查看效果

```python

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def init_network():

    # 第一层(隐藏层)
    ## 第一个神经元参数
    w11_1 = np.random.rand()
    b1_1 = np.random.rand()
    ## 第二个神经元参数
    w12_1 = np.random.rand()
    b2_1 = np.random.rand()

    # 第二层(输出层)
    # 只有一个输出神经元
    w11_2 = np.random.rand()
    w21_2 = np.random.rand()
    b1_2 = np.random.rand()
    return w11_1,b1_1,w12_1,b2_1,w11_2,w21_2,b1_2

# 前向传播
def foward_progation(xs):
    z1_1 = w11_1 * xs + b1_1
    a1_1 = sigmoid(z1_1)

    z2_1 = w12_1 * xs + b2_1
    a2_1 = sigmoid(z2_1)

    z1_2 = w11_2 * a1_1 + w21_2*a2_1+b1_2
    a1_2 =  sigmoid(z1_2)
    return a1_2,z1_2,a2_1,z2_1,a1_1,z1_1


w11_1,b1_1,w12_1,b2_1,w11_2,w21_2,b1_2 = init_network()
a1_2, z1_2, a2_1, z2_1, a1_1, z1_1 = foward_progation(xs)

plt.title('Size-Toxicity Function', fontsize=12)
plt.xlabel('Bean Size')
plt.ylabel('Toxicity')
plt.scatter(xs, ys)
plt.plot(xs,a1_2)

```

训练：梯度下降、反向传播

```python
w11_1, b1_1, w12_1, b2_1, w11_2, w21_2, b1_2 = init_network()
alpha = 0.03

for _ in range(5000):
    for i in range(n):
        x = xs[i]
        y = ys[i]
        # 先来一次前向传播
        a1_2, z1_2, a2_1, z2_1, a1_1, z1_1 = foward_progation(x)

        # 反向传播
        e = (y-a1_2)**2
        deda1_2 = -2*(y-a1_2)
        da1_2dz1_2 = a1_2*(1-a1_2)
        dz1_2dw11_2 = a1_1
        dz1_2dw21_2 = a2_1
        dz1_2db1_2 = 1

        dedw11_2 = deda1_2*da1_2dz1_2*dz1_2dw11_2
        dedw21_2 = deda1_2*da1_2dz1_2*dz1_2dw21_2
        dedb1_2 = deda1_2*da1_2dz1_2*dz1_2db1_2

        dz1_2da1_1 = w11_2
        da1_1dz1_1 = a1_1 * (1 - a1_1)
        dz1_1dw11_1 = x
        dz1_1db1_1 = 1
        dedw11_1 = deda1_2*da1_2dz1_2*dz1_2da1_1*da1_1dz1_1*dz1_1dw11_1
        dedb1_1 = deda1_2 * da1_2dz1_2 * dz1_2da1_1 * da1_1dz1_1 * dz1_1db1_1

        dz1_2d2_1 = w21_2
        da2_1dz2_1 = a2_1 * (1 - a2_1)
        dz2_1dw12_1 = x
        dz2_1db2_1 = 1
        dedw12_1 = deda1_2*da1_2dz1_2*dz1_2da1_1*da2_1dz2_1*dz2_1dw12_1
        dedb2_1 = deda1_2 * da1_2dz1_2 * dz1_2da1_1 * da2_1dz2_1 * dz2_1db2_1

        w11_1 = w11_1 - alpha * dedw11_1
        w12_1 = w12_1 - alpha * dedw12_1
        b1_1 = b1_1 - alpha * dedb1_1
        b2_1 = b2_1 - alpha * dedb2_1
        w11_2 = w11_2 - alpha * dedw11_2
        w21_2 = w21_2 - alpha * dedw21_2
        b1_2 = b1_2 - alpha * dedb1_2
```

![隐藏层-20220810201349-nsnfny9](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/06.隐藏层：神经网络为什么working/隐藏层-20220810201349-nsnfny9.gif)