# 机器学习：最后一节课也是第一节课

不知不觉间我们已经来到了本次旅程的最后一课，我们好像学到了很多，又好像什么都没有学到。当然这就是本次课程的出发点，用最简单和直接的方式揭示概念和方法，帮助入门者在细碎又纷乱的机器学习神经网络知识体系中梳理出一条脉络，而避免陷入细节的泥淖。那在这最后一节课我们先从头开始review一下我们所学的内容，并适当的补充一些之前课程中为了简化理解而舍弃的东西，当然只做简单的介绍，正如本次课程的标题一样，最后一节课也是第一节课，希望通过这些介绍，同学们可以自己去探索和研究。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/14.机器学习：最后一节课也是第一节课/20220815005026_2022-08-15.png)​

## 回顾过去课程

我们从深海生物小蓝开始介绍了人工智能启蒙阶段的**McCulloch-Pitts神经元模型和Rosenblatt感知器**，然后我们引入了线性回归，介绍了**代价函数**的概念，并在一个简单的抛物线形状的代价函数上介绍了**梯度下降算法**，我们说梯度下降算法能够在不断的训练中让误差代价逐渐向最低点波动，同时相比于正规方程有不会一次性占用太多的计算和存储资源，这是一种在海量数据上用时间换空间的思想。那除了我们所提到的传统梯度下降算法以外，人们为了改进学习的效果，又在此之上发明了很多**改进的版本**，他们都在一定程度上提高了训练的效率：

* **基于动量的梯度下降算法**：人们观察到梯度下降的过程和一个小球做下坡运动很相似，于是引入了动量的概念，得到了基于动量的梯度下降算法。
* **自适应算法**：像AdaGrad和RMSProp，这样能够根据学习过程自我调节学习率。
* **Adam算法**：同时结合了动量和自适应学习率。

随后我们开始介绍反向传播的概念，从简单的线性回归问题开始，我们看到了数据前**向传播**进行预测，误差代价**反向传播调整权重和偏置参数**——这个一个现在神经网络工作模式的雏形。随即我们介绍了神经网络中非常重要的，我称之为灵魂的—— **激活函数**，正是非线性激活函数，让神经网络摆脱了线性系统的约束，从而产生了解决复杂问题的能力，这是激活函数的主要作用。

而sigmoid的作为激活函数一般在输出层可以作为二分类问题的预测输出，而在多层神经网络的隐藏层中，我们采用ReLU激活函数，而通过一个加入激活函数的神经元，我们介绍了反向传播利用链式法则更一般的行为。

在讲述**多层神经网络**之后，我们完整的看见了反向传播在多层网络上的普遍行为，以及为什么深度神经网络能够working的原理。

最后我们介绍了**keras框架**。基于keras框架，我们简单的分析了**卷积神经网络**和**循环神经网络**的工作原理。

## 人工智能、机器学习、深度学习之间的关系

当然同学们可能已经发现了我们的课程从第六节课就开始主要讨论深度学习领域的知识，实际上深度学习只是机器学习的一个分支。人工智能、机器学习、深度学习之间的关系，实际上是这样的：

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220814231402-bph484e.png)​

早在上个世纪50年代，人们基于让让机器产生人类一样智能这样美好的愿望，提出了人工智能的概念，所以一切试图做到这一点的都可以看作是人工智能技术。

比如在人工智能早期曾十分流行的人工智能三大流派之一的**符号主义**——人们自己总结规则，然后通过类似if else的方法堆砌成一个**专家系统**，这也属于人工智能领域，而且是人工智能领域非常重要的一部分。近些年来符号主义中的知识图谱在在很多智能问答应用中还发挥着很重要的作用，但是这种符号主义手段对人工的消耗极大，每一个规则都需要人工录入，机器无法自主学习。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220814231500-42brms2.png) 

所以为了解决这个问题，人们提出了机器学习的想法，这时候我们不再需要给机器逐个录入规则本身，而是让机器自己在数据中学习到规则。

所以**一切试图做到这一点的都可以看作是机器学习技术**。

除了我们所学的深度学习，机器学习领域，还有像SVM、随机森林、贝叶斯分类等等。

其实深度学习发展的很早，但受制于算力等客观因素的限制，在很长一段时间里都被人大放异彩，不像机器学习中的SVM那样一度成为主流，但随着近些年来计算机硬件算力的不断提升和互联网发展多年的数据积累，深度学习终于迎来了他的春天。深度学习最初的灵感来自于人脑中不计其数相互连接的神经元，所以**深度神经网络属于典型的人工智能三大流派之一的连接主义**。当然这是对大脑的仿生，而不是复刻，就像飞机是对鸟儿的仿生，而不是复刻一样，面对真实的大脑，我们不能忽视深度神经网络和它之间的联系，但也不能过分的强调这种联系。​

2016年AlphaGo打败李世石，可谓是深度学习的里程碑事件，从此以后一直到现在深度学习，全面占领机器学习乃至人工智能领域的说法就一直不绝于耳。但个人观点是人工智能机器学习每个领域流派**都有自己独特的优势和不足**，只不过目前而言很多时候深度学习的优势恰好大一点，但全面取代未免有点危言耸听，比如我曾参与过的一个智能QA项目，在这个过程中我发现对于商用QA系统生成的答案，其实并没有很人性化的回答到问题，但是表述的准确性和正确性往往比所谓的智能更加重要，所以业内普遍还是偏向于使用符号主义中的知识图谱技术，而不是用深度学习让机器善于整活。

除了深度学习以外，目前机器学习领域中还有另外一项振奋人心的技术，强化学习。**强化学习的灵感来自于人工智能三大流派之一的行为主义**，让一个智能体不断的采取不同的行动，改变自己的状态和环境进行交互，从而获得不同的奖励。我们只需要设计出合适的奖励规则，智能体就能在不断的试错中取得合适的策略。强化学习近些年来也得到了很多的应用。从AlphaGo开始到近期腾讯的觉悟系统，通过强化学习训练的游戏，AI已经让人类选手开始在moba游戏中深感绝望。当然像觉悟这样的AI在强化学习中也加入了深度学习的部分，也就是所谓的深度强化学习。​

近些年来人工智能机器学习领域随着算力数数据和从业者的不断增加，正在不断的涌现着一些十分有趣的想法，等待着大家去探索和发现。

## 应该怎样继续学习

好的，简单的回顾和扩展之后，那我们来聊聊在这个入门课程之后，同学们如果想要继续学习应该是什么样的思路。当然大家的背景和目的都不太一样，所以只讲讲个人的想法和建议，大家还是结合自身的实际情况决定如何去做，就像很多技术领域一样，往往可以把这个领域的知识体系简单的分为两层，**底层实现和上层应用**，而上层应用中往往随着该领域的发展，也会出现很多经过验证的行之有效的经典方法。​

我们举个可能不是十分贴合的例子，比如编程语言中Java这个体系，jvm虚拟机、字节码技术构成了Java体系的底层实现，并通过Java语言向上提供应用的接口，而像spring、mybatis等框架以及各种常用的库，则是人们在多年的实践中总结而成能高效用于生产的经典上层实现。那在实现一个具体任务的时候，Java程序员往往会直接使用这些框架和库，而他们往往也能够应对绝大多数的问题。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220814231952-a5tr03d.png)​

同样在在深度学习领域我们学习过像梯度下降，反向传播、CNN、RNN以及我们课上并没有详细说明的，其他的一些基本原理则构成了现代神经网络的底层实现。

而像LetNet5、LSTM、GRU以及我们课上没有说过的Alexnet、VGG、ResNet、Yolo等等则是在神经网络发展的过程中经过检验而行之有效的模型。同样这些经典的网络模型在很多常见的场景，比如语音识别、自然语言处理、图像识别等领域中都有很不错的效果，所以如果你想要用神经网络实现一个具体的任务，那么你应该首先考虑这些已有的经典网络模型，就像我们使用Spring开发Java项目一样，是很自然的选择。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220814232142-7eiuu3j.png)​​

而我们为了提高自己Java项目的开发水平，可能需要去熟悉框架的实现，好消息是我们可以阅读他们的源码，只要你想你就能知道所有的细节，而坏消息是这些代码往往非常的庞杂，配合文档和资料也需要长时间的学习和研究，同样为了提高对神经网络应用的水平，我们可能需要去熟悉这些经典的网络模型，最好的方法就是阅读他们的论文，好消息是这些论文一般不会特别的长，内容也相对单一，很快就可以看一遍，但坏消息是这些论文往往不会附赠源码，一般都是数学公式和图表，阅读他们的门槛可能更高。但另外一个好消息就是对于这些经典的网络结构目前网络上已经有很多人写博客，做了更加通俗的解读。

比如我们上一节课中提到了一篇关于LSTM著名博客，[Understanding LSTM Networks ](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)，就要比看LSTM的原论文要轻松许多。有些文章甚至会逐步的贴出相应的代码，比如对于LeNet5和ALexnet，随便在网上一搜就能找到一大批关于他们论文的解读和用keras实现的代码，所以**对于这些经典网络的学习并不是一件特别困难的事情**。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220814232530-91pw6ki.png)​

当然人工智能作为一门正处于高速发展的学科，每段时间社区都会有新的idea被提出，有些可能是变革性的，有些可能只是一个小的修补，不论怎样，**如果你希望了解这些新的想法，那么唯一的方法就是自己去看这些原始的论文**，我知道这可能比较艰难，因为因为很多论文都是对想法进行简单的描述，然后给出一些公式，而且一般不会详细的说明这些公式每一步推导的细节，最后贴出测试效果的图表，而且并不会附赠源码。​

所以为了提高阅读的效率，那只能是多看排出那些写的特别晦涩的论文，一般来说一个领域内的论文看的多了，也就能够慢慢的培养出一点感觉。当然这里还有一个很重要的问题，那就是你对机器学习神经网络的底层实现有足够的了解，不仅仅是概念上的，还有数学上的。当然如果你并不是想做机器学习神经网络相关的研究工作，而只是想把它用到自己的实际问题上，那倒是不必研究的如此深刻，**在理解了大致的工作原理之后，去学习使用那些经典的模型就好**。正如我们在学习编程语言的时候，其实不是特别的深入了解计算机的底层实现，也可以写出不错的程序，但是如果是从事像操作系统这样的计算机方面的研究工作，那么深入的学习则是不可避免的。

好的，那我们的分享就到这里，是时候和大家说再见，最后愿大家在人工智能领域不断的学习和进步，并利用好这门技术，在各自的领域乘风破浪找到自己想要的未来，谢谢大家。

> 撒花完结
>