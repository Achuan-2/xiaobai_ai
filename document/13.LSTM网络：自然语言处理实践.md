# LSTM网络：自然语言处理实践

上节课我们已经完整的讲述了利用词嵌入，把句子转化为词 向量序列的详细过程。最后我们说因为无法忽视语言数据在时间上的关联性，所以我们的神经网络必须要有处理这种关联的能力，那现在我们就来看看如何做到这一点，为了讲解上的清晰，我们我们不再考虑**词嵌入**这个数据的预处理操作，就假设句子里的每个词都已经被处理成了一个合适的300d词向量。那上节课我们采用**全连接神经网络**做预测的时候，是<u>把所有的词向量平铺开，然后送入</u>。那现在我们就来改造一下神经网络的工作模式。

## RNN循环神经网络的结构

首先为了让改造过程的讲解简明一些，我们把网络结构图的画法做个简化，忽略具体的神经元之间的连线，用箭头表示数据的输入和输出，然后我们把这个句子中每个词向量分别命名为x1、x2、x3和x4。第一步先把这个句子的第一个词向量x1作为这个隐藏层的输入，直到输出a1，这时候还没完，还不到最后输出预测值的时候，我们把这个输出值a1保存起来。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815005410-q0i40vs.png "第一步")​

第二步再拉出句子的第二个词，向量x2和这个第一个词向量输出的结果a1一起作为这一步的输入，得到本次的输出a2，继续保存这个a2。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815005456-bsabtd4.png "第二步")​

第三步再拿出句子的第三个词向量x3，同样和上一次的输出a2一起作为本次的输入得到输出a3。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815005600-f7yyo6q.png "第三步")​

一直到最后一步：句子的最后一个词向量x4和a3一起输入得到输出a4，此时我们才让a4经过输出层**得到最后的预测输出**。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815005613-plofz3h.png "最后一步")​

这样一个句子，每个词对最后预测输出的影响就在每一次的保存并和下一步数据的共同作用中持续到了最后，这样序列中的数据就产生了循环的特点，所以我们把这样改造的神经网络称之为**循环神经网络RNN**。很简单吧，当然其中有一些细节可能比较繁琐一点，首先循环神经网络中的激活函数多采用<u>双曲正切函数tanh</u>，而不是relu。当然也可以使用relu，像keras这种编程框架中的循环神经网络默认使用的激活函数就是tanh。

然后循环神经网络中的第一步不像后面那样有上一个输出反送回来的数据一起作为输入，但为了保证每一步操作的统一性，我们一般也会**手动添加一个共同的输入**（a0），比如一个全0的向量。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815005755-ycjm8rv.png)​

最后是这个循环神经网络的结构图，我们这么画是为了让大家看到如何从一个普通的全连接层改造成为RNN结构，但是大家一般喜欢把输入x画在下面，最后的输出画在上面，中间的返送箭头画在侧边，这么做图的好处就是能够很方便的把网络在时间上逐步的行为在空间上展开，而展开后就能很清楚的展示出**每个时间步骤上神经网络的工作流程**。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815005843-st2stn9.png "把输入x画在下面，最后的输出画在上面，中间的返送箭头画在侧边")​

比如我们这句话有4个词，第一个词向量x1和a0一起输入得到1个输出a1，当然对于1个分类问题，这一步我们不需要最后的预测输出，所以我们把输出部分删除，只保留激活值a1，然后第二个词向量x2和a1一起输入得到第二个输出a2，同样不需要预测输出，只保留激活值a2。注意这个结构图并不是另外一个，而是和第一个是同一个，只不过是在时间点的第二步的时候的样子，这样展开的画法就保留了每一步数据流转的过程，对于读者而言就能很清晰的知道事情的细节，但你要明确的知道这样画出来的两个图**是在时间上的行为，并不是在空间上的行为**。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815005926-8rx0j8u.png "第二个词向量x2和a1一起输入得到第二个输出a2")​​

同样第三个词向量x3和a2一起输入得到第三个输出a3，我们再在右边画一个x4和a3得到a4再画一个，到此这句话就结束了，我们再让a4在通过输出层经过sigmoid或者softmax得到最后的分类预测输出，这样我们就把在时间上的每一步的行为在空间上展开了，如此我们就能清晰的看出来循环神经网络的前向传播的过程。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815010054-z5ohl23.png "循环神经网络的前向传播的过程")​

而反向传播则是从最后的输出层开始，把误差从相反的方向依次从后往前传播，当然这个反向传播的过程也是按照时间进行的，换句话说我们让时光逐步倒流进行反向传播，详细的过程我们就不展开了，现在的深度学习变成框架会帮我们处理得很好。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815010136-q6ja9h5.png "反向传播的过程")​

所以**为什么这样的循环神经网络就可以应对在时间上彼此依赖的序列问题呢**？我们用一个简单的例子粗略的来解释一下，比如对于“这个视频好看”肯定是正面的评论，那么把这个词的词向量输入将产生正向的分类结果，而在“好看”前加一个“不”肯定是一个负面评价，“不”这个否定词的输出结果和下一个词向量“好看”合并起来作为输入，那么通过训练就有机会把这个正面词汇好看给消解掉，从而让最后的输出值改变，从而产生负向分类的效果。同样如果好看前面是“非常”这个词，那么“非常”作为一个用来增强状态的副词，输出的结果和下一个词“好看”合并起来作为输入就有机会把这个正面词汇好看给增强，让让最后正向的概率值更高。同样如果是“非常”“不”“好看”三个词，那么“非常”这个词的输出和“不”一起输入就会增强“不”的输出，而这个被增强的“不”的输出在和“好看”一起输入的时候，就会对这个正面词汇产生更强的消解，得到更加负面的输出。所以**循环神经网络可以在一定程度上应对这种在时间上有依赖的序列问题**。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815010238-z7frfly.png "循环神经网络可以在一定程度上应对这种在时间上有依赖的序列问题")​

当然我们同样可以用多个这样的RNN结构形成一个多层的循环神经网络，比如我们在这一层之后再加一个一模一样的RNN结构，当然这时候第一层就不再连接输出层了，与此同时我们第一层中的每一步的激活值，除了返送回本层的下一步的输入以外，还需要作为第二层对应时间点的输入，这里还是要强调一下，千万不要被这个在时间上平铺开的图误导：<u>第一层的输入并不是一次性送入第二层的，而是在时间上一步一步送入的</u>。我们来过一遍。为了区分第一层和第二层，我们把这些符号都加上层号标识L1和L2，第一步拿出句子中的第一个词，向量x1和aL1_0合并输入第一层得到输出aL1_1。aL1_1作为第二层的输入和第二层的aL2_0合并输入第二层，得到第二层的输出aL2_1。第二步拿出句子的第二个次向量x2和第一层上一步保存的aL1_1合并输入第一层得到输出aL1_2。aL1_2作为第二层的输入和第二层上一步保存的aL2_1合并输入第二层得到第二层的输出aL2_2。第三步拿出句子的第三个次向量x3和第一层上一步保存的aL1_2合并输入第一层得到输出aL1_3。aL1_3作为第二层的输入和第二层上一步保存的的aL2_2输入第二层得到第二层的输出aL2_3，以此类推一直到最后一步，我们再让第二层最后一步的输出，通过输出层得到最后的分类预测输出。

![](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/166047880358127252776-20220814224943-86r5bp5.png "多层的循环神经网络")​

循环神经网络虽然也可以堆叠成多层的，但是人们轻易不会构造的太深。一般来说2~3层就可以了，因为循环神经网络会在时间上展开，所以网络结构将会变得很大，训练起来相对于其他的结构更加的困难。

## 长依赖的问题：引入LSTM

虽然这种标准的循环神经网络可以在一定程度上应对这种在时间上有依赖的序列问题，但是对于长依赖的问题效果就不太好了。​

我们举个例子，假如有这么一句话，“上海电视台记者在北京街头采采访了一名来自四川的年轻人，他性格开朗热爱生活，家乡有一种著名的动物叫做____”。如果要预测这句话中最后缺失的那个词是什么，但看后面部分作为具有代表性的著名动物，老虎、扬子鳄、袋鼠等等都是合理的。但是根据前面可知地点在四川，那这个词大概率是熊猫而不是其他的动物，但难搞的是四川这个词距离后面非常的远，换句话说依赖的路径十分的长，标准的RNN结构在这种常依赖问题上表现并不好。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815010822-sptv3qa.png "【长依赖问题举例】上海电视台记者在北京街头采采访了一名来自四川的年轻人，他性格开朗热爱生活，家乡有一种著名的动物叫做")​

所以为了解决这个问题，人们在循环神经网络上做了进一步的改造，其中最为著名的就是**LSTM长短时记忆网络**（Long Short Term Memory Network, LSTM）。为了清晰的解构LSTM的工作原理，我们还是从一个简单的标准RNN结构开始慢慢改造。并不复杂，你可以把这个过程看作是在玩一个小孩子搭积木的游戏，那我们先用搭积木的角度去看看一个标准的RNN结构的细节，给理解LSTM结构热身。这是一个标准的RNN结构中的某一步，上一步的激活输出at-1和这一次的词向量xt合并在一起，作为本次的输入，这样交汇了两条线就可以看成一种<u>数据汇合积木块</u>，他表示把两部分数据合并成一个比如这里的at-1和xt，那么经过交汇之后就被合并到了一一张量之中，同样有合就有分，如果是从一条线分为两条线，那么就表示把这条线上的数据拷贝为两分，进入不同的数据线路，可以看作一种<u>数据分离的积木块</u>。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815011046-yyu6c46.png)​

然后这个合并的数据先经过线性运算，再通过tanh激活函数输出本次的激活值的at，这就是一个全连接神经网络层的工作模式。我们用一个矩形表示这样的一个网络层在矩形上标明tanh，说明这一层使用的是tanh激活函数。我们按照激活函数的名字，把这一层简称为<u>tanh层</u>，这个tanh层同样可以作为一个积木块，它表示一个使用tanh激活函数的神经网络层。同样的道理，如果是一个使用sigmoid的激活函数的网络层就称之为<u>δ层</u>，也可以看作是一个积木块，我们再把输出at向上再引出一条头目。如果本次需要输出一个预测值，那么就把at送入一个输出层，当然如果本次不需要，那么就不送，这样我们就从拼积木的角度画出了一个标准RNN结构的细节的数据流转和计算的结构图。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815011207-7ghthsb.png)​

那在后续的改造中我们还会用到一些其他的积木块。我们先来明确一下，首先是<u>加法积木块</u>，用圈中一个加号表示a和b两个数据通过它之后输出就是a加b。和它类似的，还有一个<u>乘法积木块</u>，用圈中一个称号表 a和b两个数据通过它之后就输出a乘b。还有一种<u>椭圆的函数及模块</u>，用一个椭圆并在上面标注相应的函数名，这就很简单的就表示数据通过，它就被相应的函数运算一下输出，比如a通过tanh就输出tanh(a)。要注意的是这和tanh层不一样，tanh层在tanh激活函数之前还包括了一个线性运算。好的，在明确了全部积木块之后，我们就来看看**LSTM结构相比于标准的RNN结构到底发生了哪些改变**？

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815011332-hoo14c0.png)​​

首先LSTM结构中的输出再次经过一个tanh函数，而原先的输出则变成了一个叫做**细胞状态Ct**的东西，这个细胞状态就是LSTM结构能够应对长依赖问题的关键，因为它<u>能够让网络具有记忆和遗忘的效果</u>。这样既然输出变成了细胞状态和输出两个部分，那么作为一个循环中的某一步，这个结构的输入必然相应的也就变成了上一步的细胞状态和上一步的输出。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815011408-zncismj.png "结构的输入变成了上一步的细胞状态和上一步的输出")​

我们继续改造。本次输入数据也不仅仅依靠上一次的输出和本次的词向量，而把上一个细胞状态也一起作为输入，我们用加法积木把这两个部分加在一起，这就是LSTM主要的数据流转和运算的过程。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815011450-wgzj438.png)​

而为了实现记忆和遗忘。LSTM结构使用了两个门来实现，第一个叫<u>遗忘门</u>，门的实现很简单，使用一个sigmoid的网络层，我们知道sigmoid层的输出值都是在0\~1之间，如果我们让这sigmoid层的输出和上个细胞状态值相乘，那么sigmoid的输出层为0时，结果就相当于把上个细胞值全部丢弃，换句话说全部忘记，唯一时则上次的细胞值全部保留，换句话说全部记忆，而如果处于0~1之间则相当于遗忘掉部分或者说记忆部分，而这个sigmoid层的输入则是本次的词向量和上一次的输出出合并的数据，所以这样就可以通过本次和之前数据共同决定忘记多少之前的细胞值。比如在前面那句话中遇到四川这个词的时候，就知道地点发生了变化，从而忘记北京和上海这两个地点。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815011817-4cjr08j.png)​

我们让本次的词向量和上一次输出合并的数据在再经过一个sigmoid层形成另外一个控制门，控制这个在之前标准RNN结构中用来更新的部分，换句话说这个门用来控制是否更新本次的细胞状态值，这就是第二个门——<u>更新门</u>，这样网络就会选择重要的词汇更新细胞状态，比如遇到四川这个词的时候，就选择更新我们的细胞状态。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815011855-9olzvwc.png)​

而除了记忆和遗忘这两个门以外，最终的输出还有一个<u>输出门</u>，我们说细胞状态值经过一个tanh函数之后，就是本次的输出值LSTM结构让它也被一个门控制着，这样就可以在遇到重要词汇的时候产生强输出，而不重要的时候产生弱输出。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815011614-ym1bxpv.png)​

这就是LSTM模型的工作原理，从对其原理的基本讲解可以看出来，它之所以能够应对更长的序列依赖，正是因为除了输入和输出以外，他还添加了细胞状态的概念。如果我们单独看这个细胞状态的传递路径，你就会发现只要遗忘门和记忆门训练的得当，就能让像四川这种关键词在循环的过程中被传递的很长，甚至到最后一步，从而达到长时记忆的效果，而像性格这种对预测不重要的词，也能在短时间内被遗忘，从而达到短时记忆的效果。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815011639-ns1bt4s.png)​

这就是LSTM长短时记忆这个名字的由来。Lstm网络是上个世纪90年代就已经出现了一种循环神经网络结构，在循环神经网络中有着极高的地位。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815104147-ixrskvc.png)​

后来人们又提出了各种变种的循环身体网络，很多都是基于LSTM的改进。其中最为著名的就是GRU网络，它简化了LSTM结构，很多时候效果和LSTM也很接近，所以目前大家都乐于使用GRU结构。GRU结构很简单：

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815110651-7kif85j.png)​

> GRU对LSTM做了两个大改动：
>
> 1. 将输入门（更新门）、遗忘门、输出门变为两个门：更新门（Update Gate）和重置门（Reset Gate）。将 LSTM 的“遗忘门”和“输入门”融合成了一个更新门。
> 2. 将细胞状态与输出合并为一个状态
>

这里给大家推荐一个很有名的博客，[Understanding LSTM networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)，这个博客详细的解释了LSTM，最后介绍了一些包括GRU在内的LSTM的变种结构，同学们可以在课后自己研究一下，相比于LSTM，GRU了哪些改变和简化。

## 编程实验

好的，同学们，我们开始做本节课的编程实验，那本次编程实验呢我们接着上节课的文本情感分类，采用循环神经网络看看效果。首先创建一个文件，comments_lstm.py。然后把上节课的代码复制过来，这一次啊我们的代码编写起来呢就很简单，只需要把上节课的全连接层神经网络改为循环神经网络就好。好的，我们先把这三个全连接层给删除，那因为对于循环神经网络而言，词向量矩阵不再需要平铺，所以把这个Flatten层也删除掉。

这里我们使用LSTM结构的循环神经网络，当然同学们也可以自己去查阅keras的文档，试试标准的RNN结构。​​

我们先导入keras中的LSTM模块。然后呢直接在嵌入层后边把LSTM给堆叠上去，`model.add(LSTM(128))`。这个128呢就是LSTM输出数据的维度，这里我们让它输出128维，当然我们也可以再堆叠一层LSTM，这样就有两层LSTM。一般来说呢就像我们课上说的循环神经网络不适宜堆叠的很深那深层的循环神经网络，对于我们这里的问题呢效果也不大，所以我们就用两层，当然我们课上说过，多层的循环神经网络RNN也好，LSTM也好，前面这层需要在序列的每一步都输出结果作为下一步的输入，所以我们需要给第一层的LSTM配置上`return_sequence=True`，这个 return_sequence参数呢就表示每一步都输出结果的意思，那默认值是false表示，只在最后一步输出结果，所以呢这里我们把它配置为True。好的，这样呢我们就把这个网络改造成为了一个使用LSTM结构的循环神经网络，那最后呢我们把嵌入层的trainable设置为Flase，冻结我们的嵌入层参数，那看看在不训练词向量的情况下，模型的效果我们运行一下。那最后在测试集上的准确率呢是84.9%，你看比上节课在全连接神经网络中的效果呢要好许多。

```python
# 模型构建 #
model = Sequential()
model.add(Embedding(trainable=False, input_dim=vocalen,
          output_dim=300, input_length=maxlen))
model.add(LSTM(128,return_sequences=True))
model.add(LSTM(128))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='adam', metrics=['accuracy'])
```

那我们再试试打开Embedding层的trainable，让词向量同时训练，看看有什么样的效果来运行一下。那最后呢在测试集上的准确率是85.6%。唉！这好像跟我们上节课差不多，那这是怎么回事呢？

```python
model.add(Embedding(trainable=True, input_dim=vocalen,
          output_dim=300, input_length=maxlen))
```

那是这样的，当我们把嵌入层的训练打开之后，嵌入层的参数矩阵呢就会同步参与网络的训练，而我们这里嵌入层的参数数量呢有300×9512，那就要比后面的神经网络中的参数呢还要多，所以在训练的时候啊，**这个嵌入层的参数呢就会占用很大的比重**。换句话说训练的重心呢跑到了这个词嵌入矩阵参数上，所以导致了LSTM和普通的全连接阶层呢，在打开这个嵌入层的训练之后变得差距不是很大，而我们这里的语料数据集呢又不是很丰富，所以最后的词向量训练的效果呢必然不会很好，所以我们说更常用的方法呢是使用别人在海量数据集上训练出来的词向量。

那我们就来看看如何操作，那在本次的项目文件夹里呢有这样的一个文件很大，1个多G，这是我从Github上一个叫做Chinese word victors的项目下下载下来的预训练的词向量文件，我下载的这个是用word to vector在百度百科上训练的版本，这个文件里呢有几十万个以及训练好的中文词汇，每个词向量的维度是300，在本节课的项目文件夹下，还有一个我封装好的chinese_vec.py的辅助代码，帮助大家读取这个文件，首先我们把它导入进来，import chinese_vec。然后呢我们调用一下chinese_vec的load_word_vecs这个函数。这样呢我们就把这个文件中所有的预训练的词向量给加载了出来，我们自己创建的矩阵embedding_matrix就是一个用别人训练好的词向量构成的词嵌入矩阵。

```python
word_vecs = chinese_vec.load_word_vecs(input_file='./input/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5')
embedding_matrix = np.zeros((vocalen,300))
for word, i in word_index.items():
    embedding_vector = word_vecs.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
```

这样呢我们就可以把Embedding层的嵌入矩阵呢给替换掉我们。我们配置一下Embedding层的weights参数，让它等于我们的embedding_matrix。对了，别忘记这时候我们需要把trainable设置为False，冻结这一层，让他不参与训练，因为我们已经有了预训练的词向量了，嘛那这样呢我们就完成了改造，那我们来运行一下，看看使用别人训练好的词向量的效果。

```python
# 模型构建 #
model = Sequential()
# Embedding层不训练，直接使用预训练好的词向量矩阵，得到词嵌入矩阵
model.add(Embedding(trainable=False, weights=[embedding_matrix],input_dim=vocalen,
          output_dim=300, input_length=maxlen))
model.add(LSTM(128,return_sequences=True))
model.add(LSTM(128))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='adam', metrics=['accuracy'])
```

你看这一次在测试机上的准确率呢达到了89.4%，接近90%，效果大为提升了。那同样其实我们也可以在上节课的全连接层神经网络之中也使用这种方法，不过因为改造流程呢和这里一样，那我们就不再来一遍了。​

同学们可以课后自己尝试一下，那我在课前测试的结果呢在测试机上的准确率大概是87%，那比我们的LSTM使用预训练词像量在测试机上准确率呢低2~3个百分点。好的，那我们把上节课和这节课几种方案，最后在测试机上的准确率呢我们都排列出来，那趋势啊很明显，不使用预训练词向量，也不自己训练，这样的效果呢最差，那自己训练呢能够好一点，那使用第三方预训练的词向量的效果呢最好，那同时LSTM的效果呢在几种情况下都要比普通的全连接神经网络效果要好，这就看出来了，循环神经网络确实在序列问题上技高一筹。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/LSTM网络：自然语言处理实践/image-20220815104605-h1etbmz.png)​