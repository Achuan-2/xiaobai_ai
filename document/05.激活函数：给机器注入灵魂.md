# 05.激活函数：给机器注入灵魂

通过前面的学习，我们利用梯度下降算法，在反复的前向和反向传播的训练下，已经能够让小蓝从偏离现实的直觉自动学习到符合现实的认知，但是总感觉有点怪怪的，因为这种精确拟合的方式似乎并不是一个智能体思考的时候常见的模式，仔细想想，我们人在思考的时候往往不会产生精确的数值估计，而更常做的事情是分类。比如给你一个馒头，你会说这么大的我能吃饱，而这么大的我吃不饱，我们更倾向于把馒头分为能吃饱和吃不饱这两类，而不会在大脑中构建出所谓的馒头大小和和保护程度的精确函数曲线。​

再比如我们一开始说过的一只小狗的眼睛，你可能把它分为可爱和不可爱两类，顶多再加一个吓人的分类，但是无论如何很少有人会在大脑里建立眼睛大小和可爱程度的精确离合函数，对一个东西简单的贴标签比仔细的计算更符合我们的生物本能，也就是说人类思考问题的方式往往往都是离散的分类，而不是精确的拟合。对于小蓝也是如此，若能精确的估算出不同大小的豆豆的毒性固然是好事，但是一般会采用更加干脆的分类，有毒或者没毒。假如小蓝本身有一定的抗毒性，比如最多能扛住0.8的毒性，只有超过这个毒性的豆豆才会让小蓝中毒，所以小蓝会更乐意把豆豆分为有毒和无毒两类，至于具体有多少毒，那就让小蓝里的科学家们去研究。

![20220815155913](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/20220815155913.png)​

那这里我们的数据从有毒和无毒的角度来看，豆豆其实是这样的，注意我们的纵坐标不再表示毒性的大小，而变成了有毒的概念，一表示有毒零表示无毒，不存在中间的值，所以面对这个两极分化的分类问题，我们之前的神经元预测函数模型是否还有效，很明显不行，这个函数的值可以大于1，也可以小于0，我们可能更希望让最后的输出变成这样。在一元一次函数的输出大于某个阈值的时候，固定输1，而小于这个阈值的时候固定输出0，我们可以用一个分段函数很容易实现这一点，把之前的线性函数的结果在在丢进一个分段函数中进行二次加工，而这个新加入的分段函数就是一种所谓的激活函数。
![20220815155927](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/20220815155927.png)

其实我们一开始提到了罗森布拉特感知器中也有激活函数，当时为了不造成困扰，我们并没有提及罗森布拉特感知器把线性函数的计算结果放入一个类似的符号函数中进行分类，也就是说到此为止，我们才算见到了完整的罗森布拉特感知器，当然使用这种阶跃函数作为激活函数并不十分美好，远的不说，你看这个大括号看起来就不是个容易处理的家伙。我们来看看另外一种更好的一根常用的s型函数，逻辑斯特函数，当然我们一般采用标准的逻辑斯特函数，也就是取L=1，k=1，y0=0。

![20220815160026](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/20220815160026.png)

我们来看看如何使用这个s型的激活函数，可以看出来这个函数的计算结果始终在0~1之间，他的名字逻辑也暗示了这一点，他很适合做逻辑判断也就是分类。

除此之外，还有一点它很软很圆润，有的同学或许会说了，这个圆润的函数看起来好像并没有尖锐的阶跃函数自信，毕竟它有一个渐渐上坡的过程，如果我们把它应用在我们的分类问题中，就是这样的一个效果，在靠近分界点处它不够果断，确实确实但是这种软和圆润却是一个极好的特性。我们知道利用梯度下降算法进行的参数调整学习的时候，需要对函数进行求导，也就是求斜率。我们在加入激活函数后，同样需要对它也进行求导。相比之下，如果用类似阶跃函数，虽然判断的自信满满，但是在分界点数的导数不好处理，而除了分界点，其他地方的导数总是0无法进行下降，实际上阶跃函数的导数是一个冲击函数，在分界处无穷大，而在其他地方都是0，这很不利于梯度下降的进行。

![20220815160039](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/20220815160039.png)
所以使用sigmoid这种软软的润润的处处可导导数又处处不为0的激活函数是很好的。我们来看一下，把这个圆润的函数加入小蓝的神经元作为激活函数之后，小蓝现在的想法。

这是环境中的一组痘痘，有的有毒有的无毒，这是神经元预测模型的第一部分线性函数。当然线性函数的输出并不是最终的预测结果，所以我们用z表示线性函数的输出，再把线性函数的结果z作为自变量，送入到激活函数中得到a a才是最后的预测结果，所以输入x和最终预测输出的a的函数关系是这样的。

这里对于数学基础比较好的同学，这三个图像之间的关系仔细思考一下，就应该很容易的get到，但是我们还是来简单的讲解一下。首先z和x的关系是一个标准的一元一次函数，x是自变量，z是因变量，这很简单，我们改变参数w和b的时候，这个直线会变化，然后我们把z作为深耕single的自变量，输出aaz形成一个s形曲线，那这时候我们改变w和BA和c的曲线会发生改变吗？

觉得会的在弹幕里发一，觉得不会的发二，答案是不会，因为虽然a的值根据z确定，z的值又是根据线性函数计算得来的，但是在形成z和a的曲线的时候，a才不管你的z是怎么得来的，只要你的z=0，我的a就等于0.5，只要你的z=0.2，我的a就是这么多，不会因为你的z是不同的w和b的取值下算出来的，我就改变a的值，所以a和z的曲线就是一个以z等于0的分界点的s型曲线，不会因为w和b而改变。那 A和x形成的曲线，伴随着w和b取不同值的时候，同样觉得会变化的，在弹幕里发一，觉得不会的发二，答案是会。

![20220815160048](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/20220815160048.png)

比如一开始w等于5，b=-5，那这时候x要等于1，计算出来的z才等于0，a才等于0.5。所以a和x的图形是这样的，而如果w=10，b=-5，那么x要=0.5，计算出来的z才=0，a才=0.5，而且因为w变大了，随着x的增长，z增长的快，所以这个 s型曲线也就在x方向上被压缩了。从输入x到输出a实际上也就是一个复合函数，他们通过中间的z产生关系，那我们可以通过调节w和b来改变输入x和最终的输出a的函数图形，让让这个s型曲线可以移动和伸缩。

而调整w和b的方式，就是我们之前学习的梯度下降算法，通过梯度下降算法可以最终让曲线变成这个样子非常好的一个预测。但此刻对于这个新的预测模型，我们得到方差代价函数是这样的。

![20220815160102](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/20220815160102.png)​

我的天哪这么复杂的函数，而且是一个套着一个我们该如何对w和b分别求导，然后进行梯度下降呢？当然最简单和粗暴的方法还是取一个点，然后利用定义法进行求导，当然这太麻烦了，我们有更巧妙的方法。​

为了理解上的简单，我们先不看这个复杂的新模型，还是用故事最开始的时候那个最简单却包罗万象的预测函数模型来琢磨这件事。我们曾经用定义法求过它的导数，也曾用求导公式和求导的加法乘法法则，求过它的导数。最后我们说关于复合函数的求导法则，我们日后再说，那么现在好巧不巧的就是那个当当时的日后这个函数就是一个复合函数，我们重新审视一下这个函数有没有更好的方法求e对w的倒数？​

没有思路，再看一眼，再多看一眼，你就会发现这个函数里面是一个一次函数，外面是一个二次函数，也就是说这实际上是一个复合函数，我们把内外两个函数都明确写出来，是这样的，要求这两个函数的导数就10分的easy了，底层函数的因变量h对自变量w求导师负x外层函数因变量e对自变量h求导是二h噢我的老伙计不要去想h到底是什么？在e等于h的平方这个函数中，它就是一个普通的自变量，虽然这个自变量h是里程函数的因变量，但是对于e来说它就是我的自变量，我才不管你是怎么形成的，e是因变量，这就是一个最简单的一元二次函数。

![20220815160112](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/20220815160112.png)

那么e对w的导数就是直接把这两个导数相乘，这么神奇吗？为什么是直接相乘？本课程一般来说不做数学公式的推导和证明，但是这个复合函数的求导太重要了，就是它把反向传播和梯度下降，这两个现代神经网络的基石优美的结合在了一起，所以在这里有必要做一个简单的说明。当当然我们并不是在上数学课，所以不采用微积分教科书中那种严格的，但是理解起来十分费劲的证明，我们使用一种更加简单粗暴一点都不严格，但是却很好理解的方式去。

去简单的理解一下，我们在用定义法求导的时候，采用纵坐标的差值除以横坐标的差值，而且是在差距无限小的时候，所以在这个莱布尼兹求导符号体系中，你可以认为上面这个就是因变量的差值也可以理解为微分，而下面这个是自变量的差值，他俩相处之后就是倒数。

而我们目前为止一直在研究一阶导出了问题，一阶导出具有微分不变性，所以这个 Dh可以认为和这个 dh是一样的，所以可以直接约去，约去之后你看这就是de除以dw变成了e的差值除以w的差值，这不就是一对w的倒数吗？好了到这里就可以了，但是你千万不要和你的微积分老师这么解释问题，否则任何后果本人概不负责。以后学习微积分的时候也不要参考我这里的任何讲解，我们这么讲解只是为了让你用最简单的方式认可复合函数，通过乘法进行求导的方式，这种理解方式在微积分的世界里适用范围十分有限，不过好在我们的课程里的微积分知识还没有跳出这个范围，那 h我们知道等于y减WX代入之后就可以计算出e对w的倒数。​

你说巧不巧这和我们之前计算的结果一毛一样，这不是巧不巧的问题，这就和茴香豆的回字的4种写法一样，同一个事情从不同的角度去看待巴僚，不过不同于茴香豆的回字的写法，那种迂腐，我们这里每换一种方法都会让我们的计算和理解更加舒适，定义法的计算过程很痛苦，用求导公式加法和乘法的求导法则要轻松一点，而我们使用复合函数的角度去看待问题的时候，这个求导的过程就更加的酸爽了。复合函数就像洋葱从内到外一层套着一层，同样我们对这个复合函数进行求导的方法，就像剥洋葱，从外到内，一层一层的掀开，每一层分别把上一层的因变量输出作为本层的自变量输入进行进行求导。​

而通过乘法倒数就像个链条一样，从洋葱的最外层连到最内层，最终得到最外层因变量对最内层的自变量的导数，这就是复合函数的链式法则。我们知道这种求导方法之后，对于加入c个帽的激活函数的模型，代价函数对参数w求导就变得很容易了。对于这个误差函数，我们还是先看前向传播的过程，从洋葱的最内层到最外层去构造简单的函数，再从外到内的去求导。一对a求导是一个简单的二次函数求导的问题，a对z求导是一个逻辑斯特函数求导的问题，利用它的求导公式，我们在编程实验中再来详细说明这件事情。​

Z对w求导是一个简单的一次函数求导的问题，同样z对b求导也是一个简单的依次函数求导的问题，再利用复合函数的链式法则把它们乘起来，这样就得到了e对w的倒数。我们在全向传播计算的时候只需要保存好计算的结果a那么把a带入这个式子就很容易计算出具体的结果，然后利用梯度下降去调整w同样的道理，我们也可以很容易的求出e对b的倒数值。实际上利用复合函数的链式求导法则，我们可以很容易计算出更加复杂的预测模型的代价函数在各个参数上的导数。

比如我们接下来要学习的多层感知器或者说多层神经网络，其本质就是一个嵌套的非常非常深的复合函数，当然不仅可以纵向的增加神经元的层数，也可以横向的增加一层神经元的个数，而利用复合函数求导的链式法则，把梯度下降和反向传播完美结合后，不论这个网络有多深有多宽，我们都可以用同样的套路计算出代价函数在每个神经元上全部参数的导数，然后利用梯度下降算法去调整这些参数，我们会在接下来的课程中带你实现一个最简单的多层神经网络，那时候再来看看具体的操作，而激活函数出除了可以很好的用来分类之外，也给我们的神经网络注入了灵魂。​

我们之前所说的神经元函数的模型一直采用的是线性函数，线性函数说到底能力有限，对于一个多层的神经网络，如果每个神经元都是一个线性函数，那么即使我们有很多很多的神经元构建出一个复杂的神经网络，从数学上来说，他们在一起仍然是一个线性系统，因为线性函数不论怎么叠加，结果都是一个线性函数，而激活函数是非线性的，它让我们的神经网络摆脱线性的约束，开始具备处理越来越复杂问题的能力。我们将在下一节课中详细的说明这个问题，加入激活函数神经元扩展到多个节点的网络，这时候也就可以说是更一般情况下真正的反向传播算法：把代价函数在神经网络中反向传播到各个节点，计算参数对代价函数的影响，然后调整参数。正是深度学习的开山鼻祖乔弗里辛顿和戴维诺姆特的人在1986年开始在神经网络的研究中引入的反向传播算法成为了现代神经网络的基石。​

目前深度学习又统治着整个人工智能领域，而在此之前，人们对于多层感知器深度神经网络一直持有悲观的态度，正如明斯基和帕波特在反向传播算法发明之前的1961年是这么评价的。

![20220815160124](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/20220815160124.png)
这个现在看起来平平无奇的利用本科一年级就学过的复合函数的链式求导法则的反向传播算法，却是在罗森布拉特感知器发明的，27年后才被提出。正如深度学习领域的另一位巨头人物Angel说的那样，**很多看似显而易见的想法，只有在事后才变得显而易见**。

![20220815160134](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/20220815160134.png)

## 编程实验​

好了，同学们我们开始做本节课的编程实验，本次编程实验呢我们主要来做两件事情，首先把sigmoid激活函数代入到预测模型中，然后我们知道代入激活函数的预测模型的代价函数是一个比较复杂的复合函数，所以我们使用复合函数的链式求导法则，求出代价函数在w和b上的导数，然后再进行梯度下降进行参数调整。

```python
import utils.dataset as dataset
import numpy as np
import matplotlib
from matplotlib import pyplot as plt
import matplotlib.animation as animation
# 设置matplotlib正常显示中文和负号
matplotlib.rcParams['font.sans-serif']=['SimHei']   # 用黑体显示中文
matplotlib.rcParams['axes.unicode_minus']=False  

# define the function
def sigmoid(x):
    return 1/(1+np.exp(-x))
def f(x):
    return x*w+b

# 为了制作动画
ims=[]
fig = plt.figure()


# 初始化参数
w = np.random.randn(1)
b = np.random.randn(1)
alpha =0.05

# 有了sigmoid的训练过程
for _ in range(1000):
    for i in range(n):
        x=xs[i]
        y=ys[i]

        # 使用复合函数的链式求导法则，求出代价函数在w和b上的导数
        z = x*w+b
        a = 1/(1+np.exp(-z))
        e = (y-a)**2

        deda=-2*(y-a)
        dadz=a*(1-a)
        dzdw=x
        dzdb=1

        dedw = deda*dadz*dzdw
        dedb = deda*dadz*dzdb

        # 进行梯度下降，参数调整
        w = w - alpha*dedw
        b = b - alpha*dedb
```

预测动画
![sigmoid梯度下降-20220810115944-k756f1b](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/05.激活函数：给机器注入灵魂/sigmoid梯度下降-20220810115944-k756f1b.gif)