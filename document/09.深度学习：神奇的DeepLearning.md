# 09.深度学习：神奇的DeepLearning

正如我们在第六节课中说的那样，深度学习就是不断的增加一个神经网络的隐藏层神经元，让输入的数据被这些神经元不断的抽象和理解，最后得到一个具有泛化能力的预测网络模型，而我们一般把隐藏层超过三层的网络也就称之为深度神经网络。如此说来深度学习，这个听起来相当高端大气上档次，尤其是近些年来被媒体包装的十分具有神秘感的名字，瞬间你就觉得没有那么的道貌岸然。至于网络中每个节点到底在理解什么，很难用精确的数学手段去分析，我们唯一能做的事情就是收集数据送入数据进行训练，然后期待结果。当然也不是说我们对一个深度神经网络完全不可把控，起码我们能在比如学习率、激活函数、神经元的层数和数量等等方面，调节神经网络的大致工作行为，俗称调参。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815164625-1zouip3.png)​

我们以著名的Tensorflow游乐场为例，很多初学者一开始接触神经网络的时候，大概率都会被一些神秘的力量引导到这个网站来，这样点两下就可以搭建神经网络的体验真不错，但除了真不错以外，这些参数怎么设置，神经元怎么搭建合适就很懵懂了。而经过前几节课的学习，我们在对神经网络的工作原理有了相对系统的了解之后，这些东西都将变得明朗起来。那这节课我们就来把玩把玩这个经典的入门神器get到一点调参的感觉。进入这个网站之后，我们得到这样一个页面，先简单的看一下这个页面的功能分布，这是一段说明网站功能的文字不必管它，这是设置和运行的控制区域，这是数据集选择区域，这是神经网络的可视区域。

首先我们看一下数据集的选择区域，它给了我们4种数据集，其实呢我们已经见过了其中的3种，这就是我们第七节课中说的2个特征维度的输入数据。那按照先简单后繁琐的次序，我们从头到尾的捋一遍，我们选择第三个数据集，这就是最简单的线性可分的数据，如果我们把它看成我们课上说的豆豆，那么一个维度表示的大小，一个维度表示颜色的深浅，而这两坨不同颜色的点则表示他们是否有毒，蓝色的表示有毒，在第三个毒性维度上的值是一一黄色的表示无毒，在第三个毒性维度上的值是0，所以这些数据集其实都是三维图像的俯视图，而分类的方式我们也说过，那就是让预测函数曲面的0.5等高线把它们分隔开来。我们知道对于这种线性可分的问题，使用单个的审计员就可以完成分类，所以我们点击隐藏层的设置区域的减号，把隐藏层减少到顶层，也就是说只保留一个输出层的神经元。

好的，我们再设置一下神经元的激活函数，这里他默认使用的是双曲正切函数，这个激活函数目前为止我们还没有接触到，既然没听过不熟悉不知道，那么我们还是选择熟悉的老伙计，森格帽的函数学习率是0.0三，不必修改，然后点击运行按钮运行一下，果然很轻松的就让输出预测曲面的0.5等高线，完美的分割了两位痘痘，然后我们再试试第一个数据集，这是一个圈圈，中间的蓝色痘痘是有毒的，4周的黄色痘痘是无毒的，这个圈圈数据集实际上我们在第七节课的最后就让大家思考过，需要几个神经元才能分割出他们的类型，不知道大家的思考结果如何要组合出能够分割出这种痘痘的预测曲面，也就是让它的0.5等高线在俯视图上呈现出一个闭合的圈圈，至少需要一层三个神经元的隐藏层，为什么是三个两个不行吗？我们简单的来定性分析一下，还是看图说话。一个神经元的等高线是一条直线，两个是两条直线，但遗憾的是平面中的两条直线是无法完成闭合的，但是三个可以三个神经元是三条直线可以形成一个闭合的形状，把这三个神经元的计算结果再通过一个输出层的神经元进行汇合，通过singer的激活函数修饰一下之后就可以了，如此也就可以解决这种圈圈分布数据的分类问题，圈内是一类，圈外是另一类，当然这是一种简单的定性分析，计算的细节上可能会更复杂一些。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815164829-md8lxy1.png)​

我们点击隐藏层设置区域的加号，把隐藏层增加到一层，再点击这一层的设置区域上的加号，把这一层的神经元数量增加到三个，如此就得到了一个有三个神经元的一层，一层层学习率还是0.03，不必修改运行一下。果然最后输出的预测曲面的0.5等高线也完美的分割出了这两类痘痘，同样你可以尝试把隐藏层的神经元数量降低为两个，看看还能不能完成分割。虽然答案是肯定不冷，但不妨去试试。我们再来看看第二个数据集，这实际上就是在感知器最初发展阶段困扰研究人员很多年的一个问题。

当时为了证明感知器模型并没有太大的卵用，于是人们提出了一种在计算机领域非常常见的问题，异或。异或运算是这样的，如果两个数一样，结果就是0，比如0011如果两个数不一样，结果就是一比如0110。想要分割出这种分布数据的类型，单个神经元的感知器是无法做到这一点的，如此简单的问题都无法分类，所以证明感知器并没有什么卵用，这曾是感知器发展最初阶段大家普遍持有的悲观态度，彼时人工智能神经网络陷入了持久的低谷期，但现在我们知道实际上同样使用具有包含三个神经元的隐藏层的神经网络，就可以顺利的做出分类。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815164910-q9hgx59.png)​

所以这里再给大家留一个自行思考的问题，为什么需要一层包含三个神经元的隐藏层才能够分割开？好的，我们再来看最后一种数据集，这是一个蚊香一样的螺旋型数据集，极很明显只有一层包含三个神经元的隐藏层的神经网络，在这种复杂的数据集上将表现得十分疲软，那我们就用更深神经元数量以更多的网络来尝试一下。按照我们说的一般的深度网络只超过3层，那我们就把隐藏层的数量增加到3层，每个隐藏层用4个审计员运行一下，唉好像没有反应，预测结果随着训练动也不动，这是因为我们的老伙计sigmoid的激活函数的问题。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815164930-qgug4w7.png)​

我们之前说之所以选择sigmoid的函数作为激活函数，那是因为相比于阶跃函数，sigmoid的函数处处可导，而且导数处处不为0，这样在反向传播的时候，我们知道使用梯度下降算法计算函数的导数，然后利用这个导数去修正参数，但是我们并没有说sigmoid的其实有一个很严重的问题，在sigmoid的中心位置附近，这没有啥问题，但如果一旦进入了sigmoid的函数，远离中心点位置，比如这里虽然仍旧可导导数仍旧不为0，但是导数却极其的小，这样梯度下降就很难进行。距离中心越远的位置这个导数越小离也就是所谓的梯度消失的问题，越深的神经网络就越容易出现这种梯度消失的问题，所以这个网络变得很难训练，所以目前人们普遍不在使用sigmoid的激活函数。而经常使用一种叫做Relu的激活函数。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815165008-uqb6jy3.png)​

这是一个分段函数，在线性结果z大于0的时候，输出值为z；在z小于0的时候固定为0，这样在z大于0的时候处处可导，而且不会出现梯度消失的情况。在z等0的时候，从数学上来看，这个折点确实不可导，但一般很难恰好遇到这个点，真要是遇到了那就特殊处理一下，比如认为这一点的导数是0或者1或者一个很小值，比如0.01，当然如果陷入了z小于0的部分，很有可能导致这个神经元死亡，也就是所谓的Dead Relu Problem。因为根据反向传播中的链式求导法则，如果激活函数的导数是0，那么就锁定了这个神经元上的参数梯度为0，权重无法更新，所以人们又提出了一种改进版的value函数，leaky ReLU在z小于0的时候输出不再是0，而是一个缓缓倾斜的直线，如此就可以避免死亡value问题。但有趣的是当神经网络比较复杂隐藏层比较深的时候，人们通过实践得到了经验，往往是直接使用ReLU激活函数往往也有很好的效果。当然关于梯度消失的问题，激活函数只是一个方面，还有很多因素会导致这个现象，目前也有很多相关的研究，但按照经验大多数时候使用ReLU函数都会有不错的效果，所以现在最为流行的激活函数还ReLU。

![](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/166027654147636758571-20220812135518-75n9rrl.png)​

好的，那我们就把激活函数改为relu，再来运行一下，你看预测曲面开始动作了，实际上对于我们刚才研究的圈圈分布的数据，你会发现如果使用relul激活函数，而不是sigmoid的收敛的过程会更加的迅速，实际上对于隐藏层的神经元，如果没有其他想法或者特殊的需求，relu应该作为激活函数的第一选择。当然在最后的输出层，因为sigmoid的上下线刚好在0~1之间很适合做分类，所以输出层的激活函数还是选择sigmoid。到此我们从为了分类而引入sigmoid的激活函数，然后发现激活函数的作用不仅是为了分类，而且给神经网络注入了灵魂，让它成为了能够预测复杂问题的非线性系统。​

而到现在我们又发现sigmoid似乎只似乎适合用来做分类而已，当然我们已经有了其他更好的激活函数，给神经网络注入了更有趣的灵魂，兜兜转转之间sigmoid的函数也算完成了他的使命。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815165109-fbfo6sp.png)​

而现在对于这个螺旋形的数据集经过一番训练之后，似乎也有一定的效果，但并不十分的，好，那我们就给隐藏层增加更多的神经元，每1层都改成8个，再来一次。终于我们在这个螺旋形分布的数据集上取得了非常不错的分类结果，不过我们来看一下这个网络，每个神经元上的参数都被大大小小的条件成为了不同的数值，每个神经元也产生了对输入不同的中间抽象和理解，到此我们似乎已经没有什么精确的数学手段去分析每个神经元到底是在抽象什么，在理解什么了，所以欢迎开始【炼丹】。

## 编程实验

好的，同学们，我们开始做本节课的编程实验，这次实验呢我们只做一件事情，用凯瑞斯搭建一个深度神经网络，拟合一下这个长得像蚊香一样的螺旋形数据记好。好的老规矩，我们还是先创建一个源码文件，这是我们最后一次接触小蓝了。

* 需要三层隐藏层，激活函数为ReLU；一层输出层，激活为sigmoid
* 代码

  ```python
  # 创建模型
  model = Sequential() 
  model.add(Dense(units=8, activation='relu',input_dim=2 ))
  model.add(Dense(units=8, activation='relu' ))
  model.add(Dense(units=8, activation='relu'))
  model.add(Dense(units=1, activation='sigmoid'))
  # 告诉keras使用均方误差代价函数和随机梯度下降算法SGD，学习率为0.05，评估指标选择accuracy
  model.compile(loss='mse', optimizer=SGD(learning_rate=0.05),metrics=['accuracy'])
  # 开始训练
  model.fit(X, Y, epochs=5000, batch_size=10,verbose=0)
  ```
* 拟合结果

  ![Code_bDssHYV6lx](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/Code_bDssHYV6lx-20220812131745-nq43dw5.png)​
