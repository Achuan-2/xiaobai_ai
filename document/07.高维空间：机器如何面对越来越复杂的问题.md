# 07.高维空间：机器如何面对越来越复杂的问题

​如果我们想要去判断一个人是否善于打篮球，很明显仅仅考虑他的身高特征是不合理的。虽然这个特征很关键，但还需要从其他的角度去分析，比如体重身体灵活性以及是否经常见到凌晨4:00的太阳等等因素。同样在之前的课程中，为了让大家更加清晰的了解神经网络的工作原理，一直仅仅在用豆豆大小这个特征去预测它的毒性，但现实世界往往并非如此简单。比如豆豆的毒性不仅和它的大小有关系，也和它的颜色深浅有关，当毒性只和大小有关系的时候，我们可以用一个二维的坐标系来描述这个关系。

那此刻加入一个颜色深浅的特征，该如何用图像描述呢。很明显，我们需要一个三维的坐标系给颜色的深浅也指定一个表达的维度，如此我们就可以表示出大小和颜色深浅各不相同的豆豆和毒性的关系，而只有满足某些大小和颜色深浅的豆豆才有毒，也就是说这些豆豆有毒的概率是一其他的事。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815161757-hsowbeg.png)​

而当输入多了一个颜色深浅数据之后，我们的神经元的树图也就需要接收两个输入，预测函数的线性部分需要从一元一次函数变成二元一次函数，颜色深浅成为了另外一个圆，很明显这是一个有两个自变量，一个因变量的二元一次函数，如果我们把这个函数在三维坐标系中画出来，那么就是一个平面，正如一元一次函数在二维坐标系中是一条直线一样，这个线性函数的平面经经过神经元的第二部分非线性激活函数之后，就被扭曲成为一个 ，正如一元a次函数的直线被激活函数扭曲成为一个s形曲线一样，那么这个曲面就表示当大小和颜色深浅取某个值的时候，对有毒概率的与预测结果，而当我们找到这个**预测曲面上预测值为0.5的点连接在一起，用地理里的概念来说，就是高度为0.5的等高线，你会发现谁这条直线这条直线的一边的预测结果大于0.5也就是有毒，而另一边的预测结果小于0.5也就是无毒**。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815161807-n1r0h12.png)​

而当我们从上往下看俯视图效果的时候，这个0.5的等高线也就是大家可能在很多教程或书上看到过的**类型分割线**，实际上在之前的一元数和有毒概率形成的二维函数中，也有一个0.5的等高线，只不过在二维空间中它退化成为了一个点，你可以称之为等高点，而这个等高点同样是两类豆豆的分割，点本至少是一样的，而通过不断的调整权重参数，w一、w2以及天日参数b可以让这个预测模型的0.5等高线顺利的分割出有毒和无毒两类豆豆，但如果环境中豆豆的大小颜色深度和毒性分布是这样一个情况，那就无能为力了，这就是所谓的线性不可分问题，我们一开始提到的罗森布拉特感知器便止步于此，原因我们也说过了，**单个神经元无法做到这一点，一个二元一次函数在三维空间中将形成一个面，而且必然是平面。在套上sigmoid的激活函数之后，这个面被扭曲成为曲面，然而这个曲面的0.5等高线仍旧是一个直线，而一个直线并不能分割这种弯曲的边界，我们可能需要一个曲线来实现这一点**。而解决的办法我们也说过，那就是在添加隐藏层神经元。还记得为什么吗？我们在之前单输入的二维函数中讲过，从数学上来看，我们**引入多个神经元在通过梯度下降进行调整，就可以组合出不同的形状**。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815161901-zzafnis.png)​

在这里对于二元输入，同样在引入隐藏层神经元之后，我们也就可以扭曲三维空间中的面，让他们组合出不同的局面，而0.5的分割线也就随之被扭曲成为一个曲线，而通过调整这些神经元的参数，最终把曲面扭曲成为这个样子，0.5的等高线也被扭曲成为这个样子，如此就解决了这种弯曲数据分布的分类问题。这里我个人呢有一个非常有趣的想法和大家分享一下，对于之前的一元输入的二维预测函数模型，在加入隐藏层后预测曲线被扭曲，从而得到了两个0.5的等高点。如果从空间维度的角度做类比，三维空间中的直线被扭曲之后变成了曲线，那么我们似乎可以有趣的把二维空间中的点称之为直点，而扭曲之后得到了这两个点称之为二维空间中的曲点，当然这时候是我的秘密，数学中并没有曲点的概念。

这里给大家留一个可以自行研究的小问题。 

如果豆豆的有毒概率和大小颜色深浅的分布是这样的一个圈圈。圈内的有毒，而圈外的无毒，大家想想隐藏层至少需要几个神经元才可以把分割线扭曲成为一个圈，从而实现类型分割。 

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815161956-5rk4dgj.png)​

说到这里可以想象当问题数据特征越来越多，也就是说我们采集豆豆的更多特征，比如除了大小颜色深浅以外，再加一个外壳硬度作为输入数据的时候，我们也可以推而广之，但这样我们就只能再添加一个维度去描述预测模型的图像。换句话说我们**需要在四维空间作图**，作为三维世界的我们也就心有余而力不足了，只可意会而不可言传，但是数学作为一种抽象的工具，却可以很容易的做到这一点，在数学看来，这不过就是把输入多加了一个维度而已，而输入数据有多少元素，也就是所谓的特征维度也叫数据维度。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815162020-1u3xqt2.png)​

从一维的大小数据到二维的大小颜色深浅数据再到更高维度的数据，实际上我们把豆豆的特征提取的越多，也就是说我们从更多的维度去观察问题的时候，也就能够更好的预测它的毒性。正如当我们从一个人的品格、学识、性格等各种维度去观察他的时候，才能更好的去理解他，而不是仅仅从他的身高、体重和长相，这些特征或许是他众多特征中对预测最没有用的维度。

到此当输入数据的维度越来越多的时候，我们会发现权重参数也越来越多，如果一个个的去编写他们的函数表达式未免有点麻烦和拖沓。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220815162034-vl8g2e2.png)​

所以我们将在下一节课给大家介绍专门用来处理多维度数据的数学工具——向量和矩阵。学会使用向量和矩阵在机器学习和神经网络中十分重要。​

在下一节课开始，我们将逐渐走出这些底层的原理分析，直接采用keras框架实现神经网络的搭建。彼时我们不再需要处理前向传播，也不再需要手写反向传播，甚至梯度下降的细节也会被框架隐藏起来，但我们唯一逃不脱的就是对矩阵和向量的使用，即使在日后的工作和学习中作为工程人员，而不是科研人员化身调参侠，我们还是要熟练的使用矩阵和向量处理问题。然而好消息是向量和矩阵都很简单，从工科的角度来看，可以说是众多数学分支里最简单的，而我们的课程里用到的相关知识，又是最基础和精简的部分。

## 编程实验

本节课的编程实现就只实现【两个输入特征，一个神经元】的神经网络。【两个输入特征，两个以上神经元】的网络留待下节课，直接使用keras框架进行创建和训练。（不然手写反向传播会累死＞︿＜）

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220811210603-9zhf1tv.png)​

训练数据集

```python
## Create a dataset
n = 100
xs, ys = dataset.get_beans5(n)
```

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220811145008-uof5q5m.png)​

开始训练（后续可以改成向量形式）

```python
w1 = 0.1
w2 = 0.1
b = 0.1
x1s = xs[:, 0]
x2s = xs[:, 1]

# 开始训练
def forward_propgation(x1s, x2s):
    z = w1*x1s+w2*x2s+b
    a = 1/(1+np.exp(-z))
    return a

for _ in range(500):
    for i in range(n):
        x = xs[i]
        y = ys[i]

        x1=x[0]
        x2=x[1]

        # 前向传播
        a = forward_propgation(x1, x2)
        e = (y-a)**2
        # 反向传播
        deda = -2*(y-a)
        dedz = deda*a*(1-a)
        dzdw1 = x1
        dzdw2 = x2
        dzdb = 1

        dedw1 = dedz*dzdw1
        dedw2 = dedz*dzdw2
        dedb = dedz*dzdb

        # 梯度下降

        w1 = w1-0.01*dedw1
        w2 = w2-0.01*dedw2
        b = b-0.01*dedb

plot_utils.show_scatter_surface(xs, ys, forward_propgation)

```

训练结果

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20220811145041-r2hl7ef.png)​

​
