# 循环：序列依赖问题

前面两节课我们已经学习了卷积神经网络的基本原理和应用，仔细想想之所以使用卷积运算，那是因为我们观察到一个图像数据在空间上有着不可分割的关联性。那再想一下数据除了在空间上可能出现关联性以外，也可能在时间上如此，比如气温、数据、股票数据等等，当然最典型的就是我们人类的语言。如果是语音，声波随着时间依次传入我们的耳朵，进入大脑，一句完整的音波输入之后才知道对方在说什么，如果是文字或者单词随着时间逐个通过眼睛进入大脑，读完之后才知道这句话在表达什么，所以**面对这样在时间上有关联性的数据，神经网络该如何去识别和处理**。

我们以文字举例，假如这些是某个视频评论区中的评论，现在想让神经网络识别出这些评论中来，哪些是正面的，哪些是负面的，自然就像我们第一次面对图片数据那样，首先我们要想个办法把评论文字转成计算机能够识别的数字。

## 如何把文字转成计算机能够识别的数字？

### 词典索引

我们的第一反应可能是字符的编码值，比如英文我们可以使用英文字符的的ASCII码值，把这句话转化为数字。

但在自然语言处理中最小的单位往往是词而不是单个的字母，比如单词adopt和adapt在字母层面上很像，但但却是两个意思，完全不同的词，中文可能好一点，但单个汉字的价值也远远不如词，所以我们一般把**词作为自然语言处理中最基本的单位**，那么如何把一个词转化成一个数字呢？——查词典，打开一本英文词典，分别找到nice to meet和you在词典中出现的位置。

比如nice是第六千七百八十七个，to是第两千八百四十五个，meet是第五千八百九十八个，you是第九千零三十二个，那么我们就可以把这句话中所所有的单词转化成数字，从而把这句话转化成一个向量，然后就可以送入到神经网络之中了。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113121-toh78qj.png)​

中文也是这个思路，不过不像英文那样可以自然的通过空格切割一个句子的单词，中文要麻烦一点，需要先进行分词操作。比如这个视频非常精彩，这句话就可以分词为“这个，视频，非常，精彩”4个词，分好词之后和之前一样在中文词典中找到出现的位置，把它们转化成数字，从而把这句话转化成一个向量。当然这样处理会有一个问题，比如如果是一个有1万个词汇的词典，假如“开除”和“开心”两个词，因为第一个字读音的关系在词典中出现了位置很接近，比如分别在第四千零九十八和第四千零九十九，那么这两个词在数据上看来就很相似，如果我们再把它们进行归一化操作，那么就是0.4098和0.4099这两个数值差异极小，但但遗憾的是“开除”和“开心”两个词指代的概念完全不同，这样的数据就会给我们的预测模型带来不必要的麻烦。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113212-v1ozj1t.png "词转化为词典索引")​

### onehot编码

所以在自然语言处理领域，我们需要对词汇的表示方法做进一步的处理，使用onehot编码方式对词汇进行编码似乎是个好主意，如果我们使用一个有1万个词汇的词典，那么我们把每个词都处理成为一个onehot的编码，这样第一个词编码后就是第一个元素是1，其他元素是零的一个1万维的向量，第二个词编码后是第二个元素是1，其他是0的向量，以此类推。第四千零九十八个词编码后就是第四千零九十八个元素是1，其他为0的向量，最后一个词编码后就是最后一个1是以其他是0的向量，通过onehot的编码之后，每个词都完全不一样，这样会好一点。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113312-nlks8pk.png "onehot编码")​

但问题是我们严格把每个词都变得完全不一样的同时，也丢失了词汇之间的关联性，比如猫和狗这两个词在数据上应该更加接近，苹果和西瓜更加接近，而猫和苹果的差距应该更大，但呆板的onehot编码却无法体现这一点，因为这种人为的随意且呆板的词表示方法完全不能体现语言中词语的特点，而特征提取不当的数据会让神经网络变得难以训练和泛化。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113351-d1spk7y.png)​

再者onehot的编码会让输入数据非常的大，比如我们使用一个有1万个词的词典，这样每个词的onehot的编码都是一个1万倍的向量，那么一个有4个词的句子，那么输入数据就是4万个元素。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113421-ghbzrw1.png)​

### 词向量

所以人们提出了词向量的方法，当然这属于NLP领域的知识和我们的神经网络本身没有什么关系，我们简单的了解一下，在介绍多维数数据的时候，我们说过每个维度实际上是一个事物多个角度的特征，比如豆豆数据中我们可以收集大小、颜色、深浅、硬度等等豆豆的特征构成一个输入的向量数据，那么对于一个词汇是否也可以如此呢？当然语言本身就是对现实世界的描述，词汇本来也是用来指代一个事物的，比如“狗”这个词，从语言学的角度来看它是一个名词，当然在当前的语言环境中它也有一点形容词的成分，所以这个特征值可能是0.9，大概率是个名词，而从他所指代的事物上来看是动物不是植物，然后我们还可以提取像有没有皮毛，有没有尾巴等等这样的特征，这样我们就通过提取出一个词汇的多个特征值形成了一个词向量。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113553-50tw51w.png "狗的词向量展示")​

那对于这些特征，苹果这个词的特征值分别是是名词，不是动物，是植物，当然也不一定就是植物，比如它也可以只带一个手机的品牌，所以这个值可能是0.8，8成是个植物，没有皮毛，没有尾巴等等词语的这种表示方法十分美妙不。不仅可以表示出不同的词，甚至具备了一定的推理能力。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113612-tb9mkv0.png "苹果的词向量展示")​

为了做出方便，假设我们只提取词的两个特征，也就是说用一个二维的词向量表示一个词，那么我们把一个特征提取合适的词向量集合在二维空间上画出来，最后会是这样。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113628-xuqmz71.png)​

词义更加接近的词在向量空间中更加接近，反之词义无关的词距离很远，比如动物词汇聚集在了一起，植物词汇聚集在了一起，而动物中猫狗这种家养宠和野生的豺狼虎豹相比距离又更近一些，植物里水果和蔬菜又各自聚集，而动植物之间的距离和像手机电脑这样的非生物相比又要近一些。而有趣的是在一个特征提取适当的词向量集合中，如果我们用警察这个词的词向量去减去小偷这个词的词向量得到的结果向量，和猫这个词的词向量减去老鼠这个词词向量的结果向量非常的接近，这意味着警察和小偷的关系和猫和老鼠的关系十分相似。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113712-vc3r6d0.png)​

## 词嵌入

所以使用这样的数据会让我们的模型更容易训练，也更容易泛化，这个技术在NLP中称之为**词嵌入**——把词嵌入到一个特征向量空间。那么问题来了，我们该如何提取一个词的特征，从而得到合适的词向量呢？必然不是像我这样通过揣测，然后手工编写来看一下具体的细节。简单起见，假设我们要处理的文本中只有猫狗、苹果、西瓜这4个词，我们需要得到这4个词的此向量，而且我们希望此向量有10个特征，那么我们就构造1个10×4的词嵌入矩阵，并随机初始化这个矩阵的初始值。这样这个矩阵的每一列就分别表示这4个词的10维词向量。接下来我们需要想个办法，在这个嵌入矩阵上把某个词的词向量给提取出来，非常简单，我们还是依靠onehot编码来做这件事情，我们先给4个词做onehot编码，这样我们用这个嵌入矩阵点乘1个词的onehot编码时，比如苹果这个词，那么根据矩阵的点乘性质，由于苹果的编号的编码只有第三个元素是1，其他是0，所以把嵌入矩阵的第三列给提取了出来，同样用这个嵌入矩阵点乘成其他词的onehot编码，就可以提取出各自的词向量。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815114050-c85s0dt.png "词嵌入矩阵点乘苹果的onehot编码提取出苹果的词向量")​

当然我们也可以让词向量的维度更多，换句话说提取更加丰富的特征，比如300个，那么我们就把这个嵌入矩阵改成300行就好了，同时通常我们所处理的文本语料中所包含的词汇量肯定不会是4个这么少，比如有1万个词，那么就把嵌入矩阵的列改成1万就好，这时候每个词对应一个1万维度的onehot编码向量，同样可以提取出各自的300位词向量，有了这个嵌入矩阵，我们就可以把一句话中所有的词转化为词向量。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815113902-ik0f3kb.png)​

为了展示上的清晰，假设全部语料只有2句话，统计一下一共有5个词，首先我们用全部词汇构建1个词嵌入矩阵，然后给所有的词做onehot编码，那要把第一句话中的词都转化成词向量，首先我们需要把这句话中4个词的onehot编码向量合成1个矩阵，这样让嵌入矩阵点成这个onehot矩阵，就把这个句子变成了1个词向量矩阵，每一列都是句子中对应词的词向量。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815114242-6h10ozt.png)​

如果后面接的是普通的全连接神经网络，那么就把这些词向量铺开作为输入，然后进行前项传播，最后得到输出的预测值。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815114520-e11mp41.png)​

而在反向传播的时候，因为句子词向量提取的运算形式是我们用嵌入矩阵点成onehot编码矩阵。你看这和一个普通的全连接层的线性运算部分一样，我们把它称之为**嵌入层**，就像普通的全连接层中的反向传播，把误差传递到权重矩阵并更新它一样，对于这个嵌入层误差通过反向传播可以继续传递到这个词嵌入矩阵并更新它，而如我们所说**嵌入矩阵就是词汇表的词向量集合**，所以我们的词向量就可以像卷积神经网络中的卷积和那样，在训练的时候不断学习，最后自己学习到合适的词向量表示。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815114544-3x8mruo.png)​

如果“这个视频非常好看”这句话和“这个视频非常精彩”，这句话属于同一类，那么在训练的激励下，“好看”和“精彩”这两个词的词向量数据最后必然会很接近。当然额外说一点这，10个维度中每个维度的含义，虽然我们前面说可能是，是不是名词、是不是动物、是不是植物、有没有皮毛等等，但只是举个例子，实际上每个维度值是什么？其实通过后续的训练已经很抽象了，我们可能无法明确每个特征到底是什么含义，就像卷积神经网络中最后训练出的卷积核在提取什么已经很抽象了，可能是边沿可能是轮廓也可能不是，但我们知道他肯定捕捉到的一些特征，在训练的时候顺便训练词嵌入矩阵是一种方法，但**词向量矩阵要训练的恰当合理，显而易见的是需要海量的文本和词汇**。但如果我们去构建一个具体应用时，比如我们开头说判断一个评论区评论的感情，我们可能只能得到并不多的文本，几百条或者几千条的样子。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815114854-tghcr18.png)​

但转念一想语言这东西是有共性的，比如狗这个词是名词，是动物有尾巴有皮毛，这件事情正常来说在哪里都是这样。所以**更加常见的做法**是去使用别人在海量数据上训练好的词向量数据应用到我们自己的工程中，而不是自己在这些少量的数据集上训练词向量，常见的词向量训练算法有word2vec和Glove，这两个算法细节上比较琐碎，我们的课程就不展开讲解了，你只需要知道他们都是在自然语言处理领域流行的词向量训练算法就好，所以我们可以去网上下载他人，利用这些算法训练好的数据，然后把我们自己的词嵌入矩阵给替换掉，同时在训练的时候冻结这个嵌入层，让他在我们的训练中不在更新，**这是一种迁移学习的手段**，这种站在巨人肩膀上的想法在软件过程中十分常用。

回过头来想想我们在做图像识别的时候，因为对图像特征提取的方式也具备可迁移性，所以我们可以把别人训练好的参数直接迁移到自己的工程上，然后简单的处理一些细节问题，这要比我们在只有少量数据集的情况下从零开始训练要高效许多。

以上就是我们在神经网络中进行自然语言处理时，对数据进行的预处理和组织的一般方法。把一句话转化成词向量矩阵之后，可以把它平铺开，然后送入一个全连接层中进行训练预测这句话是正面还是反面。

但正如我们开始说的那样，这种语言序列类型的数据在时间上有关联性，比如通过训练我们知道这个视频非常好看，是一个正向的评论，但如果在测试集上遇到这样的一个评论，“这个视频非常不好看”，因为有非常好看这样正面的词汇，所以神经网络很容易就把它判定为一个正面的评论，但因为有“不”这个词的修饰，所以意思就发生了完全的反转。​

我们**不能忽视语言数据在时间上的关联性**，所以我们的神经网络必须要有处理这种关联性的能力。下节课，和用卷积操作把一个神经网络改造成为适合图像数据的卷积神经网络一样，我们把神经网络改造成为适合序列数据的结构。

## 编程实验

好的，同学们我们开始做本节课的编程实验，那先说一点这两节课我们学习的序列数据和序列模型呢比前面提到的模型都要抽象一点，所以课上可能会有一些一时间不能特别明白的地方，不过没有关系，那现在呢我们就通过本节课的编程实验一点点的写代码来消化一下这节课的编程实验，我们做一个文本的情感分类任务。

这个csv文件中的内容是我在网络上找到的一个数据集，是别人在各大网购平台上扒取的网购评论数据，一共有10万多条评论，每条数据呢有三个部分，第一列是它的商品分类，我们这次的任务呢用不上，这个列是他的情感标签数据，1代表正面评价，0是负面评价，第三列呢就是他的评论文本。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815115233-d8i0j4a.png)​

那我们就将神经网络识别出哪些评论是正面的，哪些是负面的。


那为了方便同学们的编码，我在工程目录下封装了一个叫做shopping_data.py的辅助工具，用来读取和处理这些数据，那现在呢我们就一点点的把这些数据读取出来并送入训练。再打印一下这些数据的形状，看看长什么样子。你看呢训练集一共有13,276个，测试集呢有3319个，唉不是说一共有10万多个数据吗？那是因为啊我在shopping_data中做数据读取的时候，把特别长的评论(评价字数超过大于20)给过滤就掉了，这样对我们实验的模型呢要友好一点，那再者呢数据量少一点，那训练也能快一点。

```python
import utils as shopping_data
from keras_preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Flatten


# 处理爬取的数据，6w条数据，情感标签+文本数据 #
X_train, y_train, X_test, y_test = shopping_data.load_data(
    input_file='./input/online_shopping_10_cats.csv')
print(f'X_train.shape:,{X_train.shape}')
print(f'y_train.shape:,{y_train.shape}')
print(f'X_test.shape:,{X_test.shape}')
print(f'y_test.shape:,{y_test.shape}')
print(X_train[0])
print(y_train[0])
```

好的，那接下来呢我们就要把评论文本转化为词向量，然后送入到神经网络中进行训练，那按照我们课上说的，我们需要统计一下全体文本中的词汇，那构建一个词嵌入矩阵，然后给每个词呢做onehot的编码，这样让词嵌入矩阵点乘一句话的onehot编码矩阵，就能把这句话转化成一个词向量矩阵，那不过这个过程呢其实并不需要我们手动操作，keras有个叫做embedding的专门来做这件事情。

![image](https://cdn.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/循环：序列依赖问题/image-20220815115652-sxa8vio.png)​​

所以第一步把数据集中所有的文本转化为词典的索引值，这件事情呢需要我们自己来做，那我们需要一个词典，不过不是真的去找一本词典，那词典的构造过程呢一般是这样，我们用程序去遍历语料中的句子，那如果是中文呢就进行分词，在这个过程中统计全体语料上所有的词语，比如是5000个，那么就可以把这5000个词放在Python的一个数组或者字典的结构中，而你可以通过读音呢对他们进行排序，当然你可以不排序就使用随机的顺序，反正不论怎样，每个词就会在数组或者字典里有一个位置的索引。

```python
# 把数据集所有文本进行分词，获得索引词典 #
vocalen, word_index = shopping_data.createWordIndex(X_train, X_test)
""" 
shopping_data.createWordIndex为自行封装的函数。
函数模块内容就是对数据集中所有文本去除标点符号后，使用jieba分词，获得所有词列表，按出现次数倒序排序，然后使用keras分词器Tokenizer来根据顺序从1开始创建词索引。
"""
```

然后我们就可以利用这个词典把数据集中文本句子都转化为索引数据。

```python
# 获得训练和测试数据的索引表示 #
X_train_index = shopping_data.word2Index(X_train, word_index)
X_test_index = shopping_data.word2Index(X_test, word_index)
```

好的，接下来呢还有1个问题需要处理一下，因为我们的评论数据每句话的长短都不一样，有的句子是5个词，有的句子是6个词或者7个词8个词等等，那这样呢最后得到的每句话的索引向量的长短就不一样，那我们在全部训练集上就无法形成一个整齐的张量，所以呢我们需要让这些句子做一个对齐的操作。

```python
from keras_preprocessing import sequence
# 把序列按照maxlen进行对齐 #
maxlen = 25
X_train_pad = sequence.pad_sequences(X_train_index, maxlen=maxlen)
X_test_pad = sequence.pad_sequences(X_test_index, maxlen=maxlen)
"""
[ 58., 299.,  92., 400.,  37.,  39., 157., 192., 367.,  95.]
→
[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,   0,  58, 299,  92, 400,  37,  39, 157, 192, 367,  95]
"""
```

好的，这样呢我们就完成了文本数据的预处理，那接下来呢我们就把数据送入到神经网络之中。Embedding层中的参数矩阵呢就是全体词汇的词向量集合，而当我们给它配置为False不训练的时候，这些词向量不会随着训练而更新，而当我们把它配置为trainable=True的时候，词嵌入矩阵呢就会随着训练一起更新，那按照我们课上说的，显然随着训练一起更新的词嵌入矩阵的效果呢要好一些，我们这里呢先配置为不训练，那一会我们再打开做一个对比。

```python
# 模型构建 #
model = Sequential()
"""Embedding层
对测试集和训练集的词语创建特征权值矩阵，对句子索引创建onehot编码矩阵，两个矩阵点乘就是这个句子的特征矩阵。这一步可以直接使用keras的Embedding层实现
- input_dim 设置输入维度，即输入的最大词数；
- output_dim 设置输出维度，即特征维度；
"""
model.add(Embedding(trainable=False, input_dim=vocalen,
          output_dim=300, input_length=maxlen))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

```

compile这个模型，这里的损失函数和优化器呢和之前有点不一样，损失函数我们使用的是binary_crossentropy，其实看名字呢就知道这是一个适用于二分类问题的交叉熵代价函数，然后呢优化器我们使用的是adm而不是sgd。因为序列问题呢一般都比较难训练，所以我们使用这个更快的优化器。那具体的细节呢我们就不展开了，你只需要知道啊Adam是一种使用动量的自适应优化器，那比普通的sgd优化器要更快，效果要更好一点。

```python
model.compile(loss='binary_crossentropy',
              optimizer='adam', metrics=['accuracy'])
```

那再下面的代码呢就是开始训练，我们使用200个回合，p尺寸呢设置为512，那最后呢在测试集上做一个评估，好的，这就是全部的代码，那我们运行一下。

```python
# 模型训练 #
model.fit(X_train_pad, y_train, batch_size=512, epochs=200)

# 保存模型
model.save('./model/12_model.h5')
loss, accuracy = model.evaluate(X_test_pad, y_test)
print(f'loss:{loss:.4f},accuracy:{accuracy:.4f}')
```

最后在测试机场准确率呢是79%，那不是很高，那我们刚才呢把嵌入层的trainable配置成了False，不训练词嵌入矩阵，那我们现在把它打开再试试看效果。

```python
model.add(Embedding(trainable=True, input_dim=vocalen,
          output_dim=300, input_length=maxlen))
```

Ok运行一下。你看现在在测试题上准确率呢是85.6%，有了明显的提高。

最后我们在课上说，如果我们从网上下载别人在海量数据上训练好的词向量，而不是用自己这些少量的数据训练效果会更好，那我们就在下节课说完循环神经网络结构之后，一起来试试，并把它们在数据集上的表现呢做一个详细的对比，我们下节课见。